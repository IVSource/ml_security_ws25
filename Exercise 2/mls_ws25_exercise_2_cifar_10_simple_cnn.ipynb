{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293ac998",
   "metadata": {},
   "source": [
    "## MLS WiSe 2025/26 Exercise 2.1: CIFAR‑10 Simple CNN (20P)\n",
    "#### Adapted from an exercise created by Dennis Eisermann\n",
    "In this notebook you will build, train and evaluate a basic convolutional‑neural‑network classifier on the CIFAR‑10 dataset using PyTorch. Read the instructions in each task cell and fill in the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26371a",
   "metadata": {},
   "source": [
    "### Task 1 – Local Setup (0P)\n",
    "(alternatively: use \"Jupyter PyTorch\" Profile on bwJupyter)\n",
    "\n",
    "* Install/import the libraries [torch](https://pytorch.org/get-started/locally/) and torchvision. Print the versions. \n",
    "* Install on your local PC: CUDA for NVIDIA and ROCM for AMD graphic cards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe4e40c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu128\n",
      "0.22.1+cu128\n",
      "Let's do some CUDA magic, since we got it!!!\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "if torch.cuda.is_available() :\n",
    "    print(\"Let's do some CUDA magic, since we got it!!!\")\n",
    "else:\n",
    "    print(\"No love, love! CUDA ain't there.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c4181-b9be-4752-8eb4-5c4a6ef4fbc5",
   "metadata": {},
   "source": [
    "### Task 2 – Load & explore the CIFAR‑10 data (5P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f2468-2da9-4699-b06c-d24753c6c663",
   "metadata": {},
   "source": [
    "1. Configure preprocessing pipelines for training and testing with normalization and horizontal flip as augmentations. These pipelines benefit the quality of the training. (For normalization, you can take pre-computed mean/std from public sources for this dataset or calculate them youself.)\n",
    "(2P)\n",
    "\n",
    "For the discussion, not the submission: What is normalization good for and why should you do this horizontal flip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "819c2fe0-f1ec-498d-8229-6d4287b385e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "def normalize_cifar10():\n",
    "    return v2.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010]\n",
    "    )\n",
    "    \n",
    "def horiziontal_flip():\n",
    "    return v2.RandomHorizontalFlip(p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a299e29",
   "metadata": {},
   "source": [
    "What is normalization good for and why should you do this horizontal flip?\n",
    "\n",
    "Answer:\n",
    "Once the data is normalized you dont have to be so careful with optimazition steps width (learning rate). Allowing to use faster learning rates for faster achieving convergence. \n",
    "The horizontal flip is a cheap way to augment the data to double the amount of training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21f233-73d4-4bb2-afaa-095fa73735c8",
   "metadata": {},
   "source": [
    "2. Download the training and test splits from [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html). Create [Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) with a batch_size of 128 and random shuffling only for training.\n",
    "Use the already defined preporcessing pipeline to transform the dataset. These datasets will be used to train and evaluate a neural network. (1P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bf4eb3-b884-402b-b6c4-1cb36c80e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "\n",
    "testloader = DataLoader(test_dataset, shuffle=False, batch_size=128)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a4c21-1e60-4148-b2df-ec9948bada1c",
   "metadata": {},
   "source": [
    "3. Show 8 sample [images](https://matplotlib.org/stable/gallery/images_contours_and_fields/image_demo.html) and their labels from test dataset. Use subplots from [Matplotlib](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html).\n",
    "The requested graphic  should show an impression of our dataset and the performed transformations. (2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "680973ba-dcc7-4dae-8eff-aafdb44c2bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "\n",
    "inputs = test_dataset.data[:8]\n",
    "labels = test_dataset.targets[:8]\n",
    "fig = plt.figure()\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.tight_layout(w_pad=7.0)\n",
    "    plt.imshow(inputs[i,:,:,:], interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(classes[int(labels[i])]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44ce91",
   "metadata": {},
   "source": [
    "### Task 3 – Build a simple CNN (5P)\n",
    "Create a PyTorch `nn.Module` called `SimpleCNN`. A good [architecture](https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/) for CIFAR‑10:\n",
    "* Conv(3→32) → ReLU → Conv(32→64) → ReLU → MaxPool(2)\n",
    "* Conv(64→128) → ReLU → MaxPool(2)\n",
    "* Flatten → Linear(128·8·8 → 256) → ReLU → Dropout(0.3) → Linear(256 → 10)\n",
    "\n",
    "You define an image classifier neural network which takes 32x32 images as input and matches them to 10 output classes.\n",
    "Instantiate the model and move it to *GPU if available*. Print the architecture.\n",
    "\n",
    "For the discussion, not the submission: Why is this a reasonable architecture? Are there others you would try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76157ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*8*8, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256,10))\n",
    "    def forward(self, x): return self.fc(self.conv(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8681633a-93f9-4e6f-ad99-e61860941130",
   "metadata": {},
   "source": [
    "### Task 4 – Train the network (5P)\n",
    "1. Create a *cross‑entropy loss* and  an *Adam* optimizer (learning‑rate 1e‑3) . The loss describes how much the result of the classifier differs from the labels of the training data.\n",
    "The optimizer describes how much every wheight of the neural network will be adjusted in every step to move the prediction towards the training label. (1P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f42b32f-06f2-4c31-b406-271b17568690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "training_net = SimpleCNN()\n",
    "training_net.train()\n",
    "\n",
    "if torch.cuda.is_available() :\n",
    "    training_net.to('cuda')\n",
    "\n",
    "Optimizer = torch.optim.NAdam(training_net.parameters(), lr=1e-3)\n",
    "Optimizer.zero_grad()\n",
    "\n",
    "# Compute loss\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de869739-c11a-4e7d-9795-46e6e5c459b2",
   "metadata": {},
   "source": [
    "2. Load the data to the GPU. Train the CNN for 10 epochs. Print training and validation [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) for each epoch. [Saves](https://pytorch.org/tutorials/beginner/saving_loading_models.html) the best model weights to `cifar10_simplecnn.pth`.  (4P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6270dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, Loss 0.9441021084785461: \n",
      "tensor(0.9441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 1, Loss 0.7933615446090698: \n",
      "tensor(0.7934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 2, Loss 0.9121307134628296: \n",
      "tensor(0.9121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 3, Loss 1.0362728834152222: \n",
      "tensor(1.0363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 4, Loss 0.8328586220741272: \n",
      "tensor(0.8329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 5, Loss 0.9434738159179688: \n",
      "tensor(0.9435, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 6, Loss 0.9438408613204956: \n",
      "tensor(0.9438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 7, Loss 0.8135433197021484: \n",
      "tensor(0.8135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 8, Loss 1.0173168182373047: \n",
      "tensor(1.0173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 9, Loss 0.9607070684432983: \n",
      "tensor(0.9607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 10, Loss 0.8337056636810303: \n",
      "tensor(0.8337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 11, Loss 0.9189015030860901: \n",
      "tensor(0.9189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 12, Loss 1.0103645324707031: \n",
      "tensor(1.0104, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 13, Loss 0.819544792175293: \n",
      "tensor(0.8195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 14, Loss 0.8841564655303955: \n",
      "tensor(0.8842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 15, Loss 0.8438311815261841: \n",
      "tensor(0.8438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 16, Loss 0.8529148697853088: \n",
      "tensor(0.8529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 17, Loss 0.8619046211242676: \n",
      "tensor(0.8619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 18, Loss 0.907829761505127: \n",
      "tensor(0.9078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 19, Loss 0.9004374146461487: \n",
      "tensor(0.9004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 20, Loss 0.9973108172416687: \n",
      "tensor(0.9973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 21, Loss 0.8728894591331482: \n",
      "tensor(0.8729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 22, Loss 0.8105111718177795: \n",
      "tensor(0.8105, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 23, Loss 0.9122218489646912: \n",
      "tensor(0.9122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 24, Loss 0.9935157895088196: \n",
      "tensor(0.9935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 25, Loss 0.9276720285415649: \n",
      "tensor(0.9277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 26, Loss 0.8831213712692261: \n",
      "tensor(0.8831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 27, Loss 0.8436959385871887: \n",
      "tensor(0.8437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 28, Loss 0.8675621747970581: \n",
      "tensor(0.8676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 29, Loss 0.8464910984039307: \n",
      "tensor(0.8465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 30, Loss 0.9042593836784363: \n",
      "tensor(0.9043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 31, Loss 0.7203143835067749: \n",
      "tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 32, Loss 0.9092004895210266: \n",
      "tensor(0.9092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 33, Loss 0.7870392203330994: \n",
      "tensor(0.7870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 34, Loss 0.7892385721206665: \n",
      "tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 35, Loss 0.9389335513114929: \n",
      "tensor(0.9389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 36, Loss 0.8622770309448242: \n",
      "tensor(0.8623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 37, Loss 0.9836679100990295: \n",
      "tensor(0.9837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 38, Loss 0.8727173805236816: \n",
      "tensor(0.8727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 39, Loss 0.8273484110832214: \n",
      "tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 40, Loss 0.9796819090843201: \n",
      "tensor(0.9797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 41, Loss 0.8372258543968201: \n",
      "tensor(0.8372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 42, Loss 0.8967409729957581: \n",
      "tensor(0.8967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 43, Loss 0.9176896810531616: \n",
      "tensor(0.9177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 44, Loss 0.8529314994812012: \n",
      "tensor(0.8529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 45, Loss 0.8169847726821899: \n",
      "tensor(0.8170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 46, Loss 1.0008188486099243: \n",
      "tensor(1.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 47, Loss 0.8715035319328308: \n",
      "tensor(0.8715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 48, Loss 0.7046188116073608: \n",
      "tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 49, Loss 0.8225298523902893: \n",
      "tensor(0.8225, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 50, Loss 0.770824670791626: \n",
      "tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 51, Loss 0.8661365509033203: \n",
      "tensor(0.8661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 52, Loss 0.8296722769737244: \n",
      "tensor(0.8297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 53, Loss 0.7861405611038208: \n",
      "tensor(0.7861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 54, Loss 1.0911903381347656: \n",
      "tensor(1.0912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 55, Loss 0.7911744713783264: \n",
      "tensor(0.7912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 56, Loss 0.7166225910186768: \n",
      "tensor(0.7166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 57, Loss 0.845760703086853: \n",
      "tensor(0.8458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 58, Loss 0.9679359197616577: \n",
      "tensor(0.9679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 59, Loss 0.7656418681144714: \n",
      "tensor(0.7656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 60, Loss 0.8474776744842529: \n",
      "tensor(0.8475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 61, Loss 1.011957049369812: \n",
      "tensor(1.0120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 62, Loss 0.9180200099945068: \n",
      "tensor(0.9180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 63, Loss 0.8313376903533936: \n",
      "tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 64, Loss 0.9586700797080994: \n",
      "tensor(0.9587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 65, Loss 0.8338630199432373: \n",
      "tensor(0.8339, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 66, Loss 0.8469192385673523: \n",
      "tensor(0.8469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 67, Loss 0.7571668028831482: \n",
      "tensor(0.7572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 68, Loss 0.8243569731712341: \n",
      "tensor(0.8244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 69, Loss 0.9046650528907776: \n",
      "tensor(0.9047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 70, Loss 0.8795959949493408: \n",
      "tensor(0.8796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 71, Loss 0.7389501929283142: \n",
      "tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 72, Loss 0.760575532913208: \n",
      "tensor(0.7606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 73, Loss 0.8407931327819824: \n",
      "tensor(0.8408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 74, Loss 0.9431846737861633: \n",
      "tensor(0.9432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 75, Loss 0.9266518354415894: \n",
      "tensor(0.9267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 76, Loss 0.8700738549232483: \n",
      "tensor(0.8701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 77, Loss 0.7932019233703613: \n",
      "tensor(0.7932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 78, Loss 0.8876661658287048: \n",
      "tensor(0.8877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 79, Loss 0.9773544073104858: \n",
      "tensor(0.9774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 80, Loss 0.9987243413925171: \n",
      "tensor(0.9987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 81, Loss 0.9563791751861572: \n",
      "tensor(0.9564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 82, Loss 0.9341182112693787: \n",
      "tensor(0.9341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 83, Loss 0.8480952978134155: \n",
      "tensor(0.8481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 84, Loss 0.792310893535614: \n",
      "tensor(0.7923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 85, Loss 0.9731775522232056: \n",
      "tensor(0.9732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 86, Loss 0.8030192852020264: \n",
      "tensor(0.8030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 87, Loss 0.8652845621109009: \n",
      "tensor(0.8653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 88, Loss 0.9151213765144348: \n",
      "tensor(0.9151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 89, Loss 1.071539044380188: \n",
      "tensor(1.0715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 90, Loss 0.8816865086555481: \n",
      "tensor(0.8817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 91, Loss 0.874575138092041: \n",
      "tensor(0.8746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 92, Loss 0.9104741811752319: \n",
      "tensor(0.9105, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 93, Loss 0.8891268968582153: \n",
      "tensor(0.8891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 94, Loss 0.7479692101478577: \n",
      "tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 95, Loss 1.0404446125030518: \n",
      "tensor(1.0404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 96, Loss 0.8524296283721924: \n",
      "tensor(0.8524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 97, Loss 0.8333890438079834: \n",
      "tensor(0.8334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 98, Loss 1.0444196462631226: \n",
      "tensor(1.0444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 99, Loss 1.2977873086929321: \n",
      "tensor(1.2978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 100, Loss 0.8190697431564331: \n",
      "tensor(0.8191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 101, Loss 0.8457328081130981: \n",
      "tensor(0.8457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 102, Loss 0.8668415546417236: \n",
      "tensor(0.8668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 103, Loss 0.9632384181022644: \n",
      "tensor(0.9632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 104, Loss 0.8691133260726929: \n",
      "tensor(0.8691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 105, Loss 0.9035579562187195: \n",
      "tensor(0.9036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 106, Loss 0.9434086084365845: \n",
      "tensor(0.9434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 107, Loss 1.0229582786560059: \n",
      "tensor(1.0230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 108, Loss 0.8565427660942078: \n",
      "tensor(0.8565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 109, Loss 0.7646362781524658: \n",
      "tensor(0.7646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 110, Loss 0.9250056743621826: \n",
      "tensor(0.9250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 111, Loss 1.055893063545227: \n",
      "tensor(1.0559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 112, Loss 0.8757695555686951: \n",
      "tensor(0.8758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 113, Loss 0.7495245933532715: \n",
      "tensor(0.7495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 114, Loss 0.8657850027084351: \n",
      "tensor(0.8658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 115, Loss 0.868955671787262: \n",
      "tensor(0.8690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 116, Loss 0.8296583294868469: \n",
      "tensor(0.8297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 117, Loss 1.0858112573623657: \n",
      "tensor(1.0858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 118, Loss 0.9063254594802856: \n",
      "tensor(0.9063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 119, Loss 0.9486404657363892: \n",
      "tensor(0.9486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 120, Loss 0.9445355534553528: \n",
      "tensor(0.9445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 121, Loss 0.9559400677680969: \n",
      "tensor(0.9559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 122, Loss 0.9813286066055298: \n",
      "tensor(0.9813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 123, Loss 0.8542686104774475: \n",
      "tensor(0.8543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 124, Loss 1.005041480064392: \n",
      "tensor(1.0050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 125, Loss 0.8613744378089905: \n",
      "tensor(0.8614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 126, Loss 0.7810123562812805: \n",
      "tensor(0.7810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 127, Loss 0.949268639087677: \n",
      "tensor(0.9493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 128, Loss 0.8091979026794434: \n",
      "tensor(0.8092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 129, Loss 0.8323955535888672: \n",
      "tensor(0.8324, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 130, Loss 0.9305464625358582: \n",
      "tensor(0.9305, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 131, Loss 0.964857280254364: \n",
      "tensor(0.9649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 132, Loss 0.8568282127380371: \n",
      "tensor(0.8568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 133, Loss 1.1117548942565918: \n",
      "tensor(1.1118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 134, Loss 0.8753595352172852: \n",
      "tensor(0.8754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 135, Loss 0.9153242111206055: \n",
      "tensor(0.9153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 136, Loss 0.7689712643623352: \n",
      "tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 137, Loss 0.8774201273918152: \n",
      "tensor(0.8774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 138, Loss 1.0665886402130127: \n",
      "tensor(1.0666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 139, Loss 0.8134242296218872: \n",
      "tensor(0.8134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 140, Loss 1.0311365127563477: \n",
      "tensor(1.0311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 141, Loss 0.8812891840934753: \n",
      "tensor(0.8813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 142, Loss 0.8269340395927429: \n",
      "tensor(0.8269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 143, Loss 0.942995011806488: \n",
      "tensor(0.9430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 144, Loss 0.7596655488014221: \n",
      "tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 145, Loss 1.0119102001190186: \n",
      "tensor(1.0119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 146, Loss 0.894771158695221: \n",
      "tensor(0.8948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 147, Loss 0.9073101878166199: \n",
      "tensor(0.9073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 148, Loss 0.934033215045929: \n",
      "tensor(0.9340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 149, Loss 0.8697777986526489: \n",
      "tensor(0.8698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 150, Loss 0.8939430117607117: \n",
      "tensor(0.8939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 151, Loss 0.9452815651893616: \n",
      "tensor(0.9453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 152, Loss 0.7916620373725891: \n",
      "tensor(0.7917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 153, Loss 0.6520382761955261: \n",
      "tensor(0.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 154, Loss 0.8333208560943604: \n",
      "tensor(0.8333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 155, Loss 0.8044225573539734: \n",
      "tensor(0.8044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 156, Loss 1.0399209260940552: \n",
      "tensor(1.0399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 157, Loss 1.0018713474273682: \n",
      "tensor(1.0019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 158, Loss 0.8642057180404663: \n",
      "tensor(0.8642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 159, Loss 0.913490891456604: \n",
      "tensor(0.9135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 160, Loss 1.050758719444275: \n",
      "tensor(1.0508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 161, Loss 0.7243154644966125: \n",
      "tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 162, Loss 1.0373131036758423: \n",
      "tensor(1.0373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 163, Loss 0.9238417744636536: \n",
      "tensor(0.9238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 164, Loss 0.8704854249954224: \n",
      "tensor(0.8705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 165, Loss 0.9356698989868164: \n",
      "tensor(0.9357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 166, Loss 0.8208157420158386: \n",
      "tensor(0.8208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 167, Loss 0.7261521816253662: \n",
      "tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 168, Loss 0.7941346764564514: \n",
      "tensor(0.7941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 169, Loss 1.089604377746582: \n",
      "tensor(1.0896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 170, Loss 0.8476357460021973: \n",
      "tensor(0.8476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 171, Loss 1.1428742408752441: \n",
      "tensor(1.1429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 172, Loss 0.9569326043128967: \n",
      "tensor(0.9569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 173, Loss 0.8995150923728943: \n",
      "tensor(0.8995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 174, Loss 0.8765365481376648: \n",
      "tensor(0.8765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 175, Loss 0.8267742991447449: \n",
      "tensor(0.8268, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 176, Loss 1.1661032438278198: \n",
      "tensor(1.1661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 177, Loss 0.9107483625411987: \n",
      "tensor(0.9107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 178, Loss 0.9012175798416138: \n",
      "tensor(0.9012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 179, Loss 0.9051492810249329: \n",
      "tensor(0.9051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 180, Loss 0.8770605325698853: \n",
      "tensor(0.8771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 181, Loss 0.9285799264907837: \n",
      "tensor(0.9286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 182, Loss 1.0355125665664673: \n",
      "tensor(1.0355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 183, Loss 0.9764699935913086: \n",
      "tensor(0.9765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 184, Loss 0.9731853008270264: \n",
      "tensor(0.9732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 185, Loss 0.8226559162139893: \n",
      "tensor(0.8227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 186, Loss 0.9381263256072998: \n",
      "tensor(0.9381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 187, Loss 0.9434270262718201: \n",
      "tensor(0.9434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 188, Loss 0.9675344228744507: \n",
      "tensor(0.9675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 189, Loss 0.9700023531913757: \n",
      "tensor(0.9700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 190, Loss 0.8578757047653198: \n",
      "tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 191, Loss 0.9136836528778076: \n",
      "tensor(0.9137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 192, Loss 0.7368659377098083: \n",
      "tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 193, Loss 0.8230153322219849: \n",
      "tensor(0.8230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 194, Loss 0.9697304964065552: \n",
      "tensor(0.9697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 195, Loss 0.8407624959945679: \n",
      "tensor(0.8408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 196, Loss 0.9014213681221008: \n",
      "tensor(0.9014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 197, Loss 0.960381031036377: \n",
      "tensor(0.9604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 198, Loss 0.9017095565795898: \n",
      "tensor(0.9017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 199, Loss 0.7830275297164917: \n",
      "tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 200, Loss 0.8952265381813049: \n",
      "tensor(0.8952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 201, Loss 0.9018018245697021: \n",
      "tensor(0.9018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 202, Loss 0.8927261233329773: \n",
      "tensor(0.8927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 203, Loss 0.8381713628768921: \n",
      "tensor(0.8382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 204, Loss 1.0548733472824097: \n",
      "tensor(1.0549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 205, Loss 0.9255613684654236: \n",
      "tensor(0.9256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 206, Loss 0.7976260185241699: \n",
      "tensor(0.7976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 207, Loss 0.7546846866607666: \n",
      "tensor(0.7547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 208, Loss 0.9110420942306519: \n",
      "tensor(0.9110, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 209, Loss 0.838286280632019: \n",
      "tensor(0.8383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 210, Loss 0.8364536166191101: \n",
      "tensor(0.8365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 211, Loss 0.9443827271461487: \n",
      "tensor(0.9444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 212, Loss 0.9669870138168335: \n",
      "tensor(0.9670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 213, Loss 0.8223551511764526: \n",
      "tensor(0.8224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 214, Loss 0.8869218826293945: \n",
      "tensor(0.8869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 215, Loss 1.2377487421035767: \n",
      "tensor(1.2377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 216, Loss 0.9609270691871643: \n",
      "tensor(0.9609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 217, Loss 0.8845401406288147: \n",
      "tensor(0.8845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 218, Loss 0.9056286215782166: \n",
      "tensor(0.9056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 219, Loss 0.794594943523407: \n",
      "tensor(0.7946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 220, Loss 0.8598105907440186: \n",
      "tensor(0.8598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 221, Loss 0.8267250061035156: \n",
      "tensor(0.8267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 222, Loss 0.7954440116882324: \n",
      "tensor(0.7954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 223, Loss 0.8785876035690308: \n",
      "tensor(0.8786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 224, Loss 0.8385128974914551: \n",
      "tensor(0.8385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 225, Loss 0.940949022769928: \n",
      "tensor(0.9409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 226, Loss 0.9583443403244019: \n",
      "tensor(0.9583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 227, Loss 1.0062546730041504: \n",
      "tensor(1.0063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 228, Loss 0.8807101845741272: \n",
      "tensor(0.8807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 229, Loss 0.8408696055412292: \n",
      "tensor(0.8409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 230, Loss 0.8974612951278687: \n",
      "tensor(0.8975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 231, Loss 1.0847971439361572: \n",
      "tensor(1.0848, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 232, Loss 1.023441195487976: \n",
      "tensor(1.0234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 233, Loss 1.0217039585113525: \n",
      "tensor(1.0217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 234, Loss 0.8451306819915771: \n",
      "tensor(0.8451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 235, Loss 0.8314124941825867: \n",
      "tensor(0.8314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 236, Loss 0.9427196979522705: \n",
      "tensor(0.9427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 237, Loss 0.8628259301185608: \n",
      "tensor(0.8628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 238, Loss 0.9451585412025452: \n",
      "tensor(0.9452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 239, Loss 0.8020336627960205: \n",
      "tensor(0.8020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 240, Loss 0.9431527853012085: \n",
      "tensor(0.9432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 241, Loss 0.792682409286499: \n",
      "tensor(0.7927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 242, Loss 0.8130444288253784: \n",
      "tensor(0.8130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 243, Loss 1.0412660837173462: \n",
      "tensor(1.0413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 244, Loss 1.05431067943573: \n",
      "tensor(1.0543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 245, Loss 1.0508849620819092: \n",
      "tensor(1.0509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 246, Loss 0.8577466011047363: \n",
      "tensor(0.8577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 247, Loss 0.8777154088020325: \n",
      "tensor(0.8777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 248, Loss 0.8916964530944824: \n",
      "tensor(0.8917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 249, Loss 0.8700512647628784: \n",
      "tensor(0.8701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 250, Loss 0.8929961323738098: \n",
      "tensor(0.8930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 251, Loss 0.875052273273468: \n",
      "tensor(0.8751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 252, Loss 0.7861681580543518: \n",
      "tensor(0.7862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 253, Loss 0.8237231373786926: \n",
      "tensor(0.8237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 254, Loss 0.879069447517395: \n",
      "tensor(0.8791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 255, Loss 0.8818092346191406: \n",
      "tensor(0.8818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 256, Loss 0.9517198801040649: \n",
      "tensor(0.9517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 257, Loss 0.8382012248039246: \n",
      "tensor(0.8382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 258, Loss 1.098681926727295: \n",
      "tensor(1.0987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 259, Loss 0.9640370607376099: \n",
      "tensor(0.9640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 260, Loss 0.7737866044044495: \n",
      "tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 261, Loss 0.9204201102256775: \n",
      "tensor(0.9204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 262, Loss 0.9653968214988708: \n",
      "tensor(0.9654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 263, Loss 0.8661139607429504: \n",
      "tensor(0.8661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 264, Loss 0.805694043636322: \n",
      "tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 265, Loss 0.9436420798301697: \n",
      "tensor(0.9436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 266, Loss 0.8067731261253357: \n",
      "tensor(0.8068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 267, Loss 0.840404748916626: \n",
      "tensor(0.8404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 268, Loss 0.8121429085731506: \n",
      "tensor(0.8121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 269, Loss 0.8104192018508911: \n",
      "tensor(0.8104, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 270, Loss 0.9204949736595154: \n",
      "tensor(0.9205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 271, Loss 0.7660378813743591: \n",
      "tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 272, Loss 0.939027726650238: \n",
      "tensor(0.9390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 273, Loss 0.8914024233818054: \n",
      "tensor(0.8914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 274, Loss 0.6212999224662781: \n",
      "tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 275, Loss 0.9433648586273193: \n",
      "tensor(0.9434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 276, Loss 0.8802921175956726: \n",
      "tensor(0.8803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 277, Loss 0.8779155015945435: \n",
      "tensor(0.8779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 278, Loss 1.002780556678772: \n",
      "tensor(1.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 279, Loss 1.0879634618759155: \n",
      "tensor(1.0880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 280, Loss 0.8764702081680298: \n",
      "tensor(0.8765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 281, Loss 0.7335805296897888: \n",
      "tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 282, Loss 0.9729300737380981: \n",
      "tensor(0.9729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 283, Loss 1.0187480449676514: \n",
      "tensor(1.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 284, Loss 0.9195227026939392: \n",
      "tensor(0.9195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 285, Loss 0.9452721476554871: \n",
      "tensor(0.9453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 286, Loss 0.8776769042015076: \n",
      "tensor(0.8777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 287, Loss 0.8108370900154114: \n",
      "tensor(0.8108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 288, Loss 0.732957124710083: \n",
      "tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 289, Loss 1.0900567770004272: \n",
      "tensor(1.0901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 290, Loss 1.0458166599273682: \n",
      "tensor(1.0458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 291, Loss 0.863034188747406: \n",
      "tensor(0.8630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 292, Loss 0.770357608795166: \n",
      "tensor(0.7704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 293, Loss 0.9407538771629333: \n",
      "tensor(0.9408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 294, Loss 1.0824708938598633: \n",
      "tensor(1.0825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 295, Loss 0.9270709156990051: \n",
      "tensor(0.9271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 296, Loss 0.9714726805686951: \n",
      "tensor(0.9715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 297, Loss 0.840080201625824: \n",
      "tensor(0.8401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 298, Loss 0.768997848033905: \n",
      "tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 299, Loss 0.7858734726905823: \n",
      "tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 300, Loss 0.9158386588096619: \n",
      "tensor(0.9158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 301, Loss 0.9086022973060608: \n",
      "tensor(0.9086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 302, Loss 1.062807559967041: \n",
      "tensor(1.0628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 303, Loss 1.0011361837387085: \n",
      "tensor(1.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 304, Loss 1.0187994241714478: \n",
      "tensor(1.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 305, Loss 0.8875418901443481: \n",
      "tensor(0.8875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 306, Loss 0.9357811808586121: \n",
      "tensor(0.9358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 307, Loss 0.9560773372650146: \n",
      "tensor(0.9561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 308, Loss 1.1132097244262695: \n",
      "tensor(1.1132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 309, Loss 0.9493944048881531: \n",
      "tensor(0.9494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 310, Loss 0.8889065384864807: \n",
      "tensor(0.8889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 311, Loss 0.9550268650054932: \n",
      "tensor(0.9550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 312, Loss 0.942162811756134: \n",
      "tensor(0.9422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 313, Loss 0.8459655046463013: \n",
      "tensor(0.8460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 314, Loss 1.02711021900177: \n",
      "tensor(1.0271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 315, Loss 0.9701208472251892: \n",
      "tensor(0.9701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 316, Loss 0.7776843309402466: \n",
      "tensor(0.7777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 317, Loss 0.8613886833190918: \n",
      "tensor(0.8614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 318, Loss 0.8572506904602051: \n",
      "tensor(0.8573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 319, Loss 0.8743064403533936: \n",
      "tensor(0.8743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 320, Loss 1.0181233882904053: \n",
      "tensor(1.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 321, Loss 0.9679216742515564: \n",
      "tensor(0.9679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 322, Loss 0.8438652753829956: \n",
      "tensor(0.8439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 323, Loss 0.9519968628883362: \n",
      "tensor(0.9520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 324, Loss 0.8945951461791992: \n",
      "tensor(0.8946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 325, Loss 0.8348848223686218: \n",
      "tensor(0.8349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 326, Loss 1.1165196895599365: \n",
      "tensor(1.1165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 327, Loss 0.9306612610816956: \n",
      "tensor(0.9307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 328, Loss 0.8460890650749207: \n",
      "tensor(0.8461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 329, Loss 0.842379093170166: \n",
      "tensor(0.8424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 330, Loss 0.8731183409690857: \n",
      "tensor(0.8731, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 331, Loss 0.8618471026420593: \n",
      "tensor(0.8618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 332, Loss 0.9195058941841125: \n",
      "tensor(0.9195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 333, Loss 0.897203266620636: \n",
      "tensor(0.8972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 334, Loss 0.8451547026634216: \n",
      "tensor(0.8452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 335, Loss 0.8374004364013672: \n",
      "tensor(0.8374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 336, Loss 1.118989109992981: \n",
      "tensor(1.1190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 337, Loss 0.8022781610488892: \n",
      "tensor(0.8023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 338, Loss 0.9333127737045288: \n",
      "tensor(0.9333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 339, Loss 0.8769365549087524: \n",
      "tensor(0.8769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 340, Loss 0.728880763053894: \n",
      "tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 341, Loss 1.0054855346679688: \n",
      "tensor(1.0055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 342, Loss 0.7866590023040771: \n",
      "tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 343, Loss 0.9318344593048096: \n",
      "tensor(0.9318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 344, Loss 0.8051773309707642: \n",
      "tensor(0.8052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 345, Loss 0.911537766456604: \n",
      "tensor(0.9115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 346, Loss 1.0963603258132935: \n",
      "tensor(1.0964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 347, Loss 0.9157345294952393: \n",
      "tensor(0.9157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 348, Loss 0.9613549709320068: \n",
      "tensor(0.9614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 349, Loss 0.9137916564941406: \n",
      "tensor(0.9138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 350, Loss 0.902245819568634: \n",
      "tensor(0.9022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 351, Loss 0.9016915559768677: \n",
      "tensor(0.9017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 352, Loss 0.795613706111908: \n",
      "tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 353, Loss 0.7896983623504639: \n",
      "tensor(0.7897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 354, Loss 0.8640519380569458: \n",
      "tensor(0.8641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 355, Loss 0.9258003830909729: \n",
      "tensor(0.9258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 356, Loss 1.0131458044052124: \n",
      "tensor(1.0131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 357, Loss 0.9532464146614075: \n",
      "tensor(0.9532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 358, Loss 0.9093225598335266: \n",
      "tensor(0.9093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 359, Loss 0.7656751871109009: \n",
      "tensor(0.7657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 360, Loss 0.9546770453453064: \n",
      "tensor(0.9547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 361, Loss 0.9770331382751465: \n",
      "tensor(0.9770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 362, Loss 0.9261717796325684: \n",
      "tensor(0.9262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 363, Loss 0.749552845954895: \n",
      "tensor(0.7496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 364, Loss 0.7846636772155762: \n",
      "tensor(0.7847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 365, Loss 0.7961394786834717: \n",
      "tensor(0.7961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 366, Loss 0.7606809735298157: \n",
      "tensor(0.7607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 367, Loss 0.8815990090370178: \n",
      "tensor(0.8816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 368, Loss 1.1024707555770874: \n",
      "tensor(1.1025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 369, Loss 1.0255327224731445: \n",
      "tensor(1.0255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 370, Loss 0.9928106665611267: \n",
      "tensor(0.9928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 371, Loss 0.8731572031974792: \n",
      "tensor(0.8732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 372, Loss 0.876413106918335: \n",
      "tensor(0.8764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 373, Loss 0.8931086659431458: \n",
      "tensor(0.8931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 374, Loss 0.9063920974731445: \n",
      "tensor(0.9064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 375, Loss 0.9118658304214478: \n",
      "tensor(0.9119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 376, Loss 0.8133957386016846: \n",
      "tensor(0.8134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 377, Loss 0.7581037282943726: \n",
      "tensor(0.7581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 378, Loss 0.7721000909805298: \n",
      "tensor(0.7721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 379, Loss 0.8958514928817749: \n",
      "tensor(0.8959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 380, Loss 0.9174092411994934: \n",
      "tensor(0.9174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 381, Loss 0.8169293403625488: \n",
      "tensor(0.8169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 382, Loss 1.000260353088379: \n",
      "tensor(1.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 383, Loss 0.9501526951789856: \n",
      "tensor(0.9502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 384, Loss 0.871050238609314: \n",
      "tensor(0.8711, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 385, Loss 0.839636504650116: \n",
      "tensor(0.8396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 386, Loss 1.0131291151046753: \n",
      "tensor(1.0131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 387, Loss 0.8744954466819763: \n",
      "tensor(0.8745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 388, Loss 0.8093302249908447: \n",
      "tensor(0.8093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 389, Loss 0.8380672931671143: \n",
      "tensor(0.8381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0, Iteration 390, Loss 0.9639140367507935: \n",
      "tensor(0.9639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 0, Loss 0.93858402967453: \n",
      "tensor(0.9386, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 1, Loss 0.8743476271629333: \n",
      "tensor(0.8743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 2, Loss 0.8281903862953186: \n",
      "tensor(0.8282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 3, Loss 0.855751633644104: \n",
      "tensor(0.8558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 4, Loss 0.7519035339355469: \n",
      "tensor(0.7519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 5, Loss 0.8430187106132507: \n",
      "tensor(0.8430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 6, Loss 0.7788690328598022: \n",
      "tensor(0.7789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 7, Loss 0.8289535641670227: \n",
      "tensor(0.8290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 8, Loss 0.6584149599075317: \n",
      "tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 9, Loss 0.9402098059654236: \n",
      "tensor(0.9402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 10, Loss 0.8131096363067627: \n",
      "tensor(0.8131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 11, Loss 0.7319830656051636: \n",
      "tensor(0.7320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 12, Loss 0.742129385471344: \n",
      "tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 13, Loss 0.8252981305122375: \n",
      "tensor(0.8253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 14, Loss 0.8482542634010315: \n",
      "tensor(0.8483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 15, Loss 0.7592158913612366: \n",
      "tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 16, Loss 0.9225297570228577: \n",
      "tensor(0.9225, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 17, Loss 0.7390117645263672: \n",
      "tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 18, Loss 0.7267236709594727: \n",
      "tensor(0.7267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 19, Loss 0.8624627590179443: \n",
      "tensor(0.8625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 20, Loss 0.7612524032592773: \n",
      "tensor(0.7613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 21, Loss 0.7307692170143127: \n",
      "tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 22, Loss 0.9606633186340332: \n",
      "tensor(0.9607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 23, Loss 0.7094593048095703: \n",
      "tensor(0.7095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 24, Loss 0.9783706665039062: \n",
      "tensor(0.9784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 25, Loss 0.9420346021652222: \n",
      "tensor(0.9420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 26, Loss 0.8100584149360657: \n",
      "tensor(0.8101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 27, Loss 0.7069486379623413: \n",
      "tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 28, Loss 0.9532991647720337: \n",
      "tensor(0.9533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 29, Loss 0.8239333033561707: \n",
      "tensor(0.8239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 30, Loss 0.8273302316665649: \n",
      "tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 31, Loss 0.8515118956565857: \n",
      "tensor(0.8515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 32, Loss 0.7827785015106201: \n",
      "tensor(0.7828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 33, Loss 0.8771052360534668: \n",
      "tensor(0.8771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 34, Loss 0.8799720406532288: \n",
      "tensor(0.8800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 35, Loss 0.7902041673660278: \n",
      "tensor(0.7902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 36, Loss 1.0383191108703613: \n",
      "tensor(1.0383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 37, Loss 0.7278565764427185: \n",
      "tensor(0.7279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 38, Loss 0.8203954100608826: \n",
      "tensor(0.8204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 39, Loss 0.9940037727355957: \n",
      "tensor(0.9940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 40, Loss 0.7621162533760071: \n",
      "tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 41, Loss 0.8574650287628174: \n",
      "tensor(0.8575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 42, Loss 0.8582146167755127: \n",
      "tensor(0.8582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 43, Loss 0.8328553438186646: \n",
      "tensor(0.8329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 44, Loss 0.6798058152198792: \n",
      "tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 45, Loss 1.0160894393920898: \n",
      "tensor(1.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 46, Loss 0.8509930372238159: \n",
      "tensor(0.8510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 47, Loss 0.7376458048820496: \n",
      "tensor(0.7376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 48, Loss 0.8528614640235901: \n",
      "tensor(0.8529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 49, Loss 0.8696708679199219: \n",
      "tensor(0.8697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 50, Loss 0.8994364142417908: \n",
      "tensor(0.8994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 51, Loss 0.7072399854660034: \n",
      "tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 52, Loss 0.8705179691314697: \n",
      "tensor(0.8705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 53, Loss 0.9435351490974426: \n",
      "tensor(0.9435, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 54, Loss 0.7984779477119446: \n",
      "tensor(0.7985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 55, Loss 0.7487764358520508: \n",
      "tensor(0.7488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 56, Loss 0.8999307751655579: \n",
      "tensor(0.8999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 57, Loss 0.96821129322052: \n",
      "tensor(0.9682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 58, Loss 0.7751895785331726: \n",
      "tensor(0.7752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 59, Loss 0.7668930292129517: \n",
      "tensor(0.7669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 60, Loss 0.7441825270652771: \n",
      "tensor(0.7442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 61, Loss 0.9163538813591003: \n",
      "tensor(0.9164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 62, Loss 0.8998708724975586: \n",
      "tensor(0.8999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 63, Loss 0.8595526218414307: \n",
      "tensor(0.8596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 64, Loss 0.9658613801002502: \n",
      "tensor(0.9659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 65, Loss 0.8967311978340149: \n",
      "tensor(0.8967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 66, Loss 0.9659117460250854: \n",
      "tensor(0.9659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 67, Loss 0.801705539226532: \n",
      "tensor(0.8017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 68, Loss 0.7821815013885498: \n",
      "tensor(0.7822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 69, Loss 0.8743637204170227: \n",
      "tensor(0.8744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 70, Loss 0.8433297276496887: \n",
      "tensor(0.8433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 71, Loss 0.7859552502632141: \n",
      "tensor(0.7860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 72, Loss 0.9194374084472656: \n",
      "tensor(0.9194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 73, Loss 0.7090353965759277: \n",
      "tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 74, Loss 0.6826293468475342: \n",
      "tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 75, Loss 0.7913573980331421: \n",
      "tensor(0.7914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 76, Loss 0.8603145480155945: \n",
      "tensor(0.8603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 77, Loss 0.9032675623893738: \n",
      "tensor(0.9033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 78, Loss 0.9052860140800476: \n",
      "tensor(0.9053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 79, Loss 1.1464838981628418: \n",
      "tensor(1.1465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 80, Loss 0.7825210690498352: \n",
      "tensor(0.7825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 81, Loss 0.805110514163971: \n",
      "tensor(0.8051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 82, Loss 0.8573964238166809: \n",
      "tensor(0.8574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 83, Loss 0.9005903005599976: \n",
      "tensor(0.9006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 84, Loss 0.9063481092453003: \n",
      "tensor(0.9063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 85, Loss 0.6791567802429199: \n",
      "tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 86, Loss 0.7987617254257202: \n",
      "tensor(0.7988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 87, Loss 0.8535001277923584: \n",
      "tensor(0.8535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 88, Loss 0.9396833777427673: \n",
      "tensor(0.9397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 89, Loss 0.8232628703117371: \n",
      "tensor(0.8233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 90, Loss 0.8209406733512878: \n",
      "tensor(0.8209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 91, Loss 0.7858389616012573: \n",
      "tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 92, Loss 0.8665218353271484: \n",
      "tensor(0.8665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 93, Loss 0.9124370813369751: \n",
      "tensor(0.9124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 94, Loss 0.8780989050865173: \n",
      "tensor(0.8781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 95, Loss 0.7190393805503845: \n",
      "tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 96, Loss 0.8663010001182556: \n",
      "tensor(0.8663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 97, Loss 0.7937154173851013: \n",
      "tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 98, Loss 0.8706318140029907: \n",
      "tensor(0.8706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 99, Loss 0.8285285234451294: \n",
      "tensor(0.8285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 100, Loss 0.8726959824562073: \n",
      "tensor(0.8727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 101, Loss 0.8229666948318481: \n",
      "tensor(0.8230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 102, Loss 0.8438939452171326: \n",
      "tensor(0.8439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 103, Loss 0.8838585019111633: \n",
      "tensor(0.8839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 104, Loss 0.7017408609390259: \n",
      "tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 105, Loss 0.7885642647743225: \n",
      "tensor(0.7886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 106, Loss 0.8771901726722717: \n",
      "tensor(0.8772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 107, Loss 0.7632578015327454: \n",
      "tensor(0.7633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 108, Loss 0.8058899641036987: \n",
      "tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 109, Loss 0.7415054440498352: \n",
      "tensor(0.7415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 110, Loss 0.6782917976379395: \n",
      "tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 111, Loss 0.9485543966293335: \n",
      "tensor(0.9486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 112, Loss 0.9654732942581177: \n",
      "tensor(0.9655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 113, Loss 0.8425706028938293: \n",
      "tensor(0.8426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 114, Loss 0.7352961301803589: \n",
      "tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 115, Loss 0.9184720516204834: \n",
      "tensor(0.9185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 116, Loss 0.698281466960907: \n",
      "tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 117, Loss 0.8511834144592285: \n",
      "tensor(0.8512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 118, Loss 0.8141390085220337: \n",
      "tensor(0.8141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 119, Loss 1.0285804271697998: \n",
      "tensor(1.0286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 120, Loss 0.9775419235229492: \n",
      "tensor(0.9775, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 121, Loss 0.7375006079673767: \n",
      "tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 122, Loss 0.8040916919708252: \n",
      "tensor(0.8041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 123, Loss 0.8313233256340027: \n",
      "tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 124, Loss 0.8794168829917908: \n",
      "tensor(0.8794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 125, Loss 0.7147635221481323: \n",
      "tensor(0.7148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 126, Loss 0.8453651070594788: \n",
      "tensor(0.8454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 127, Loss 0.7380460500717163: \n",
      "tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 128, Loss 0.7305574417114258: \n",
      "tensor(0.7306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 129, Loss 0.8758415579795837: \n",
      "tensor(0.8758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 130, Loss 0.9124524593353271: \n",
      "tensor(0.9125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 131, Loss 0.741381824016571: \n",
      "tensor(0.7414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 132, Loss 0.8043712377548218: \n",
      "tensor(0.8044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 133, Loss 0.9479755163192749: \n",
      "tensor(0.9480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 134, Loss 0.7138742804527283: \n",
      "tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 135, Loss 0.9897819757461548: \n",
      "tensor(0.9898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 136, Loss 0.9391213059425354: \n",
      "tensor(0.9391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 137, Loss 0.6557662487030029: \n",
      "tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 138, Loss 0.9146560430526733: \n",
      "tensor(0.9147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 139, Loss 1.012465000152588: \n",
      "tensor(1.0125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 140, Loss 0.7558395862579346: \n",
      "tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 141, Loss 0.805937647819519: \n",
      "tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 142, Loss 0.8339635133743286: \n",
      "tensor(0.8340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 143, Loss 0.8404836058616638: \n",
      "tensor(0.8405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 144, Loss 0.5661758184432983: \n",
      "tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 145, Loss 0.7815988659858704: \n",
      "tensor(0.7816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 146, Loss 0.9119129180908203: \n",
      "tensor(0.9119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 147, Loss 0.8942570686340332: \n",
      "tensor(0.8943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 148, Loss 0.9343011379241943: \n",
      "tensor(0.9343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 149, Loss 0.821690022945404: \n",
      "tensor(0.8217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 150, Loss 0.7923794984817505: \n",
      "tensor(0.7924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 151, Loss 0.805400013923645: \n",
      "tensor(0.8054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 152, Loss 0.8679401874542236: \n",
      "tensor(0.8679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 153, Loss 0.8783573508262634: \n",
      "tensor(0.8784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 154, Loss 1.1254217624664307: \n",
      "tensor(1.1254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 155, Loss 0.7433404922485352: \n",
      "tensor(0.7433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 156, Loss 0.7320742607116699: \n",
      "tensor(0.7321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 157, Loss 0.8270435929298401: \n",
      "tensor(0.8270, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 158, Loss 0.897914469242096: \n",
      "tensor(0.8979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 159, Loss 0.8644441366195679: \n",
      "tensor(0.8644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 160, Loss 0.7267171144485474: \n",
      "tensor(0.7267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 161, Loss 0.7349309325218201: \n",
      "tensor(0.7349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 162, Loss 0.8017609119415283: \n",
      "tensor(0.8018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 163, Loss 0.8190301060676575: \n",
      "tensor(0.8190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 164, Loss 0.8525574803352356: \n",
      "tensor(0.8526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 165, Loss 0.9520204663276672: \n",
      "tensor(0.9520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 166, Loss 0.8002743124961853: \n",
      "tensor(0.8003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 167, Loss 1.02617609500885: \n",
      "tensor(1.0262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 168, Loss 0.8465579748153687: \n",
      "tensor(0.8466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 169, Loss 0.8341846466064453: \n",
      "tensor(0.8342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 170, Loss 0.7578408122062683: \n",
      "tensor(0.7578, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 171, Loss 0.8112156391143799: \n",
      "tensor(0.8112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 172, Loss 0.952284038066864: \n",
      "tensor(0.9523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 173, Loss 0.794762134552002: \n",
      "tensor(0.7948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 174, Loss 0.8343085646629333: \n",
      "tensor(0.8343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 175, Loss 0.8800793290138245: \n",
      "tensor(0.8801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 176, Loss 0.7691988348960876: \n",
      "tensor(0.7692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 177, Loss 0.9547044038772583: \n",
      "tensor(0.9547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 178, Loss 0.6924597024917603: \n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 179, Loss 0.7872952222824097: \n",
      "tensor(0.7873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 180, Loss 0.9492142796516418: \n",
      "tensor(0.9492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 181, Loss 0.8617337942123413: \n",
      "tensor(0.8617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 182, Loss 0.8760488033294678: \n",
      "tensor(0.8760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 183, Loss 0.8378188610076904: \n",
      "tensor(0.8378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 184, Loss 0.8155789375305176: \n",
      "tensor(0.8156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 185, Loss 0.8635692000389099: \n",
      "tensor(0.8636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 186, Loss 0.8057032227516174: \n",
      "tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 187, Loss 0.845278263092041: \n",
      "tensor(0.8453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 188, Loss 1.0238476991653442: \n",
      "tensor(1.0238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 189, Loss 0.8889214396476746: \n",
      "tensor(0.8889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 190, Loss 0.7840162515640259: \n",
      "tensor(0.7840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 191, Loss 0.9019674062728882: \n",
      "tensor(0.9020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 192, Loss 0.8786981105804443: \n",
      "tensor(0.8787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 193, Loss 0.9832770824432373: \n",
      "tensor(0.9833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 194, Loss 0.6769635677337646: \n",
      "tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 195, Loss 0.7536942362785339: \n",
      "tensor(0.7537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 196, Loss 0.7093700766563416: \n",
      "tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 197, Loss 0.7136611342430115: \n",
      "tensor(0.7137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 198, Loss 0.8929276466369629: \n",
      "tensor(0.8929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 199, Loss 0.8320230841636658: \n",
      "tensor(0.8320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 200, Loss 0.8686531782150269: \n",
      "tensor(0.8687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 201, Loss 0.7969231009483337: \n",
      "tensor(0.7969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 202, Loss 0.9686824083328247: \n",
      "tensor(0.9687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 203, Loss 0.9693168997764587: \n",
      "tensor(0.9693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 204, Loss 0.8446230888366699: \n",
      "tensor(0.8446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 205, Loss 0.7403243780136108: \n",
      "tensor(0.7403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 206, Loss 0.8773008584976196: \n",
      "tensor(0.8773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 207, Loss 0.8889837265014648: \n",
      "tensor(0.8890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 208, Loss 0.948028028011322: \n",
      "tensor(0.9480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 209, Loss 0.9015192985534668: \n",
      "tensor(0.9015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 210, Loss 0.7650288343429565: \n",
      "tensor(0.7650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 211, Loss 0.9117299914360046: \n",
      "tensor(0.9117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 212, Loss 0.7467613816261292: \n",
      "tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 213, Loss 0.841384768486023: \n",
      "tensor(0.8414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 214, Loss 0.9009652733802795: \n",
      "tensor(0.9010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 215, Loss 0.9418004751205444: \n",
      "tensor(0.9418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 216, Loss 0.8181666731834412: \n",
      "tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 217, Loss 0.7520955801010132: \n",
      "tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 218, Loss 0.8309138417243958: \n",
      "tensor(0.8309, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 219, Loss 0.7891088724136353: \n",
      "tensor(0.7891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 220, Loss 0.969328761100769: \n",
      "tensor(0.9693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 221, Loss 0.902978241443634: \n",
      "tensor(0.9030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 222, Loss 0.9377095103263855: \n",
      "tensor(0.9377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 223, Loss 0.9579217433929443: \n",
      "tensor(0.9579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 224, Loss 0.8875457048416138: \n",
      "tensor(0.8875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 225, Loss 0.8000807166099548: \n",
      "tensor(0.8001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 226, Loss 0.9609144926071167: \n",
      "tensor(0.9609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 227, Loss 0.9767102599143982: \n",
      "tensor(0.9767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 228, Loss 0.8577138185501099: \n",
      "tensor(0.8577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 229, Loss 0.9694949388504028: \n",
      "tensor(0.9695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 230, Loss 0.7754946351051331: \n",
      "tensor(0.7755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 231, Loss 0.7498934864997864: \n",
      "tensor(0.7499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 232, Loss 0.7752558588981628: \n",
      "tensor(0.7753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 233, Loss 0.9973493218421936: \n",
      "tensor(0.9973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 234, Loss 0.7599750757217407: \n",
      "tensor(0.7600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 235, Loss 0.9085296392440796: \n",
      "tensor(0.9085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 236, Loss 0.8152199387550354: \n",
      "tensor(0.8152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 237, Loss 0.8773739337921143: \n",
      "tensor(0.8774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 238, Loss 0.9206057786941528: \n",
      "tensor(0.9206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 239, Loss 0.8900022506713867: \n",
      "tensor(0.8900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 240, Loss 0.9168455600738525: \n",
      "tensor(0.9168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 241, Loss 0.804098904132843: \n",
      "tensor(0.8041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 242, Loss 0.7592548727989197: \n",
      "tensor(0.7593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 243, Loss 0.892951250076294: \n",
      "tensor(0.8930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 244, Loss 0.9441789388656616: \n",
      "tensor(0.9442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 245, Loss 0.8567020893096924: \n",
      "tensor(0.8567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 246, Loss 0.8725308775901794: \n",
      "tensor(0.8725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 247, Loss 0.8747363090515137: \n",
      "tensor(0.8747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 248, Loss 0.9478725790977478: \n",
      "tensor(0.9479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 249, Loss 0.8736035823822021: \n",
      "tensor(0.8736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 250, Loss 0.9126027226448059: \n",
      "tensor(0.9126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 251, Loss 0.9967319965362549: \n",
      "tensor(0.9967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 252, Loss 0.8545233011245728: \n",
      "tensor(0.8545, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 253, Loss 0.9633747339248657: \n",
      "tensor(0.9634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 254, Loss 0.7332058548927307: \n",
      "tensor(0.7332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 255, Loss 0.7807794809341431: \n",
      "tensor(0.7808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 256, Loss 0.7768125534057617: \n",
      "tensor(0.7768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 257, Loss 0.9176272749900818: \n",
      "tensor(0.9176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 258, Loss 0.7427282333374023: \n",
      "tensor(0.7427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 259, Loss 0.8638381361961365: \n",
      "tensor(0.8638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 260, Loss 0.7525955438613892: \n",
      "tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 261, Loss 0.8375389575958252: \n",
      "tensor(0.8375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 262, Loss 0.7860817313194275: \n",
      "tensor(0.7861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 263, Loss 0.9034103751182556: \n",
      "tensor(0.9034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 264, Loss 0.8974663615226746: \n",
      "tensor(0.8975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 265, Loss 0.7623977065086365: \n",
      "tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 266, Loss 1.09285569190979: \n",
      "tensor(1.0929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 267, Loss 0.7630395293235779: \n",
      "tensor(0.7630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 268, Loss 0.8593127727508545: \n",
      "tensor(0.8593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 269, Loss 0.9423136115074158: \n",
      "tensor(0.9423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 270, Loss 0.8207640051841736: \n",
      "tensor(0.8208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 271, Loss 0.7337061166763306: \n",
      "tensor(0.7337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 272, Loss 0.8809350728988647: \n",
      "tensor(0.8809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 273, Loss 0.8728721141815186: \n",
      "tensor(0.8729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 274, Loss 0.9449247121810913: \n",
      "tensor(0.9449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 275, Loss 0.7393457889556885: \n",
      "tensor(0.7393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 276, Loss 0.9101859927177429: \n",
      "tensor(0.9102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 277, Loss 0.8900710940361023: \n",
      "tensor(0.8901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 278, Loss 0.810850977897644: \n",
      "tensor(0.8109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 279, Loss 0.9236478805541992: \n",
      "tensor(0.9236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 280, Loss 1.032037615776062: \n",
      "tensor(1.0320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 281, Loss 0.8695983290672302: \n",
      "tensor(0.8696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 282, Loss 0.7397339344024658: \n",
      "tensor(0.7397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 283, Loss 0.8563440442085266: \n",
      "tensor(0.8563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 284, Loss 0.8758993744850159: \n",
      "tensor(0.8759, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 285, Loss 1.021049976348877: \n",
      "tensor(1.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 286, Loss 0.9429408311843872: \n",
      "tensor(0.9429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 287, Loss 0.6191866993904114: \n",
      "tensor(0.6192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 288, Loss 0.8776971101760864: \n",
      "tensor(0.8777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 289, Loss 0.897018313407898: \n",
      "tensor(0.8970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 290, Loss 0.8159044981002808: \n",
      "tensor(0.8159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 291, Loss 0.7808014154434204: \n",
      "tensor(0.7808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 292, Loss 0.7759801149368286: \n",
      "tensor(0.7760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 293, Loss 0.9578438997268677: \n",
      "tensor(0.9578, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 294, Loss 0.9543589353561401: \n",
      "tensor(0.9544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 295, Loss 0.8873324990272522: \n",
      "tensor(0.8873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 296, Loss 0.789345383644104: \n",
      "tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 297, Loss 0.6237143278121948: \n",
      "tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 298, Loss 0.8928670287132263: \n",
      "tensor(0.8929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 299, Loss 0.6795536875724792: \n",
      "tensor(0.6796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 300, Loss 0.7007017731666565: \n",
      "tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 301, Loss 0.6456199884414673: \n",
      "tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 302, Loss 0.7999842762947083: \n",
      "tensor(0.8000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 303, Loss 0.8326308727264404: \n",
      "tensor(0.8326, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 304, Loss 0.8486338257789612: \n",
      "tensor(0.8486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 305, Loss 0.9935279488563538: \n",
      "tensor(0.9935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 306, Loss 0.752709686756134: \n",
      "tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 307, Loss 0.7764222025871277: \n",
      "tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 308, Loss 0.7662505507469177: \n",
      "tensor(0.7663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 309, Loss 0.9178364276885986: \n",
      "tensor(0.9178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 310, Loss 0.8877557516098022: \n",
      "tensor(0.8878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 311, Loss 0.8837912678718567: \n",
      "tensor(0.8838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 312, Loss 0.9160803556442261: \n",
      "tensor(0.9161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 313, Loss 0.8175433874130249: \n",
      "tensor(0.8175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 314, Loss 0.825446367263794: \n",
      "tensor(0.8254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 315, Loss 0.8521133661270142: \n",
      "tensor(0.8521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 316, Loss 0.9386224746704102: \n",
      "tensor(0.9386, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 317, Loss 0.8674373626708984: \n",
      "tensor(0.8674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 318, Loss 0.9843392372131348: \n",
      "tensor(0.9843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 319, Loss 0.8446894288063049: \n",
      "tensor(0.8447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 320, Loss 0.8461967706680298: \n",
      "tensor(0.8462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 321, Loss 0.8736870884895325: \n",
      "tensor(0.8737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 322, Loss 0.8264557719230652: \n",
      "tensor(0.8265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 323, Loss 0.7419269680976868: \n",
      "tensor(0.7419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 324, Loss 0.8644397854804993: \n",
      "tensor(0.8644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 325, Loss 0.8111031651496887: \n",
      "tensor(0.8111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 326, Loss 0.8014401793479919: \n",
      "tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 327, Loss 0.8645101189613342: \n",
      "tensor(0.8645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 328, Loss 0.7984235882759094: \n",
      "tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 329, Loss 0.932503879070282: \n",
      "tensor(0.9325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 330, Loss 0.9432682394981384: \n",
      "tensor(0.9433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 331, Loss 1.000815749168396: \n",
      "tensor(1.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 332, Loss 0.7849043011665344: \n",
      "tensor(0.7849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 333, Loss 0.6964012980461121: \n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 334, Loss 0.9191011786460876: \n",
      "tensor(0.9191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 335, Loss 0.6876146197319031: \n",
      "tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 336, Loss 0.6886329650878906: \n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 337, Loss 0.770622968673706: \n",
      "tensor(0.7706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 338, Loss 0.8710440993309021: \n",
      "tensor(0.8710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 339, Loss 0.7803871035575867: \n",
      "tensor(0.7804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 340, Loss 0.889079213142395: \n",
      "tensor(0.8891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 341, Loss 0.8340641856193542: \n",
      "tensor(0.8341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 342, Loss 0.7538778781890869: \n",
      "tensor(0.7539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 343, Loss 0.8494357466697693: \n",
      "tensor(0.8494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 344, Loss 0.814635157585144: \n",
      "tensor(0.8146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 345, Loss 1.1255171298980713: \n",
      "tensor(1.1255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 346, Loss 0.9460623860359192: \n",
      "tensor(0.9461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 347, Loss 0.8220104575157166: \n",
      "tensor(0.8220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 348, Loss 0.8502654433250427: \n",
      "tensor(0.8503, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 349, Loss 0.6321334838867188: \n",
      "tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 350, Loss 0.9535507559776306: \n",
      "tensor(0.9536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 351, Loss 0.87649005651474: \n",
      "tensor(0.8765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 352, Loss 0.6537247896194458: \n",
      "tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 353, Loss 0.9063505530357361: \n",
      "tensor(0.9064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 354, Loss 0.8619887232780457: \n",
      "tensor(0.8620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 355, Loss 0.712714433670044: \n",
      "tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 356, Loss 0.9184932112693787: \n",
      "tensor(0.9185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 357, Loss 0.7653328776359558: \n",
      "tensor(0.7653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 358, Loss 0.7382676601409912: \n",
      "tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 359, Loss 0.9074629545211792: \n",
      "tensor(0.9075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 360, Loss 0.8156589865684509: \n",
      "tensor(0.8157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 361, Loss 0.9302069544792175: \n",
      "tensor(0.9302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 362, Loss 0.899167537689209: \n",
      "tensor(0.8992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 363, Loss 0.8406622409820557: \n",
      "tensor(0.8407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 364, Loss 0.909557580947876: \n",
      "tensor(0.9096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 365, Loss 0.8983577489852905: \n",
      "tensor(0.8984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 366, Loss 0.8795304894447327: \n",
      "tensor(0.8795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 367, Loss 0.9533839225769043: \n",
      "tensor(0.9534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 368, Loss 0.8378749489784241: \n",
      "tensor(0.8379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 369, Loss 0.79067063331604: \n",
      "tensor(0.7907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 370, Loss 0.7880780696868896: \n",
      "tensor(0.7881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 371, Loss 0.7039546966552734: \n",
      "tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 372, Loss 0.8243915438652039: \n",
      "tensor(0.8244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 373, Loss 0.7863819003105164: \n",
      "tensor(0.7864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 374, Loss 0.8867491483688354: \n",
      "tensor(0.8867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 375, Loss 1.0092464685440063: \n",
      "tensor(1.0092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 376, Loss 0.8387694954872131: \n",
      "tensor(0.8388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 377, Loss 0.7400621175765991: \n",
      "tensor(0.7401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 378, Loss 0.7998534440994263: \n",
      "tensor(0.7999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 379, Loss 0.716875433921814: \n",
      "tensor(0.7169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 380, Loss 0.9640780091285706: \n",
      "tensor(0.9641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 381, Loss 0.6259245872497559: \n",
      "tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 382, Loss 0.7870843410491943: \n",
      "tensor(0.7871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 383, Loss 0.8370184898376465: \n",
      "tensor(0.8370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 384, Loss 0.8780025243759155: \n",
      "tensor(0.8780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 385, Loss 0.8810832500457764: \n",
      "tensor(0.8811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 386, Loss 0.6807817816734314: \n",
      "tensor(0.6808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 387, Loss 1.100620985031128: \n",
      "tensor(1.1006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 388, Loss 0.8201851844787598: \n",
      "tensor(0.8202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 389, Loss 0.9252903461456299: \n",
      "tensor(0.9253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, Iteration 390, Loss 0.6854805946350098: \n",
      "tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 0, Loss 0.838039219379425: \n",
      "tensor(0.8380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 1, Loss 0.8754759430885315: \n",
      "tensor(0.8755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 2, Loss 0.7771904468536377: \n",
      "tensor(0.7772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 3, Loss 0.7641229629516602: \n",
      "tensor(0.7641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 4, Loss 0.8292497992515564: \n",
      "tensor(0.8292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 5, Loss 0.8375455737113953: \n",
      "tensor(0.8375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 6, Loss 0.6745883822441101: \n",
      "tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 7, Loss 0.6651883125305176: \n",
      "tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 8, Loss 0.9532164335250854: \n",
      "tensor(0.9532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 9, Loss 0.7236445546150208: \n",
      "tensor(0.7236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 10, Loss 0.6435710787773132: \n",
      "tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 11, Loss 0.8820968866348267: \n",
      "tensor(0.8821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 12, Loss 0.7869553565979004: \n",
      "tensor(0.7870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 13, Loss 0.6170642375946045: \n",
      "tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 14, Loss 0.8160527944564819: \n",
      "tensor(0.8161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 15, Loss 0.7357300519943237: \n",
      "tensor(0.7357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 16, Loss 0.6751294732093811: \n",
      "tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 17, Loss 0.8759025931358337: \n",
      "tensor(0.8759, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 18, Loss 0.8906670808792114: \n",
      "tensor(0.8907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 19, Loss 0.7782366275787354: \n",
      "tensor(0.7782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 20, Loss 0.750066876411438: \n",
      "tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 21, Loss 0.7917698621749878: \n",
      "tensor(0.7918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 22, Loss 0.6317638158798218: \n",
      "tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 23, Loss 0.6235237717628479: \n",
      "tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 24, Loss 0.6557576656341553: \n",
      "tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 25, Loss 0.7667869329452515: \n",
      "tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 26, Loss 0.8659365773200989: \n",
      "tensor(0.8659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 27, Loss 0.8537111878395081: \n",
      "tensor(0.8537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 28, Loss 0.6465904712677002: \n",
      "tensor(0.6466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 29, Loss 0.92179274559021: \n",
      "tensor(0.9218, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 30, Loss 0.6275880336761475: \n",
      "tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 31, Loss 0.7459182739257812: \n",
      "tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 32, Loss 0.715705156326294: \n",
      "tensor(0.7157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 33, Loss 0.8765351176261902: \n",
      "tensor(0.8765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 34, Loss 0.7973323464393616: \n",
      "tensor(0.7973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 35, Loss 0.7410557270050049: \n",
      "tensor(0.7411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 36, Loss 0.7385363578796387: \n",
      "tensor(0.7385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 37, Loss 0.8638724088668823: \n",
      "tensor(0.8639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 38, Loss 0.7778500914573669: \n",
      "tensor(0.7779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 39, Loss 0.7637853622436523: \n",
      "tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 40, Loss 0.6454573273658752: \n",
      "tensor(0.6455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 41, Loss 0.7600385546684265: \n",
      "tensor(0.7600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 42, Loss 0.795907735824585: \n",
      "tensor(0.7959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 43, Loss 0.7636430859565735: \n",
      "tensor(0.7636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 44, Loss 0.7903414964675903: \n",
      "tensor(0.7903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 45, Loss 0.9727011322975159: \n",
      "tensor(0.9727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 46, Loss 0.8810466527938843: \n",
      "tensor(0.8810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 47, Loss 0.9060803055763245: \n",
      "tensor(0.9061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 48, Loss 0.9211727380752563: \n",
      "tensor(0.9212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 49, Loss 0.5680199265480042: \n",
      "tensor(0.5680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 50, Loss 0.9607272744178772: \n",
      "tensor(0.9607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 51, Loss 0.7974141240119934: \n",
      "tensor(0.7974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 52, Loss 0.8647376298904419: \n",
      "tensor(0.8647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 53, Loss 0.7854562997817993: \n",
      "tensor(0.7855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 54, Loss 0.814695417881012: \n",
      "tensor(0.8147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 55, Loss 0.7487937808036804: \n",
      "tensor(0.7488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 56, Loss 0.6892945170402527: \n",
      "tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 57, Loss 0.758630633354187: \n",
      "tensor(0.7586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 58, Loss 0.8654607534408569: \n",
      "tensor(0.8655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 59, Loss 0.6968161463737488: \n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 60, Loss 0.7139177322387695: \n",
      "tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 61, Loss 0.7390045523643494: \n",
      "tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 62, Loss 0.6890268921852112: \n",
      "tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 63, Loss 0.6925476789474487: \n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 64, Loss 0.9888069033622742: \n",
      "tensor(0.9888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 65, Loss 0.8582724332809448: \n",
      "tensor(0.8583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 66, Loss 0.8114646077156067: \n",
      "tensor(0.8115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 67, Loss 0.8303201794624329: \n",
      "tensor(0.8303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 68, Loss 0.9757364392280579: \n",
      "tensor(0.9757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 69, Loss 0.900459349155426: \n",
      "tensor(0.9005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 70, Loss 0.8043200373649597: \n",
      "tensor(0.8043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 71, Loss 0.8410767912864685: \n",
      "tensor(0.8411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 72, Loss 0.8590059280395508: \n",
      "tensor(0.8590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 73, Loss 0.679165244102478: \n",
      "tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 74, Loss 0.7276356816291809: \n",
      "tensor(0.7276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 75, Loss 0.7942144274711609: \n",
      "tensor(0.7942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 76, Loss 0.7485336065292358: \n",
      "tensor(0.7485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 77, Loss 0.9355343580245972: \n",
      "tensor(0.9355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 78, Loss 0.7364488840103149: \n",
      "tensor(0.7364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 79, Loss 0.7629422545433044: \n",
      "tensor(0.7629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 80, Loss 0.8102688193321228: \n",
      "tensor(0.8103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 81, Loss 0.7249569892883301: \n",
      "tensor(0.7250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 82, Loss 0.718101441860199: \n",
      "tensor(0.7181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 83, Loss 0.7627037763595581: \n",
      "tensor(0.7627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 84, Loss 0.9336824417114258: \n",
      "tensor(0.9337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 85, Loss 0.6098132729530334: \n",
      "tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 86, Loss 0.8623436689376831: \n",
      "tensor(0.8623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 87, Loss 0.6250138878822327: \n",
      "tensor(0.6250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 88, Loss 0.7530924081802368: \n",
      "tensor(0.7531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 89, Loss 0.6554470062255859: \n",
      "tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 90, Loss 0.702787458896637: \n",
      "tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 91, Loss 0.6977049708366394: \n",
      "tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 92, Loss 0.9229483008384705: \n",
      "tensor(0.9229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 93, Loss 0.7628312110900879: \n",
      "tensor(0.7628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 94, Loss 0.7497988939285278: \n",
      "tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 95, Loss 1.0170525312423706: \n",
      "tensor(1.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 96, Loss 0.6762214303016663: \n",
      "tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 97, Loss 0.7981458306312561: \n",
      "tensor(0.7981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 98, Loss 0.7598558664321899: \n",
      "tensor(0.7599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 99, Loss 0.7804687023162842: \n",
      "tensor(0.7805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 100, Loss 0.8382364511489868: \n",
      "tensor(0.8382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 101, Loss 0.7728455662727356: \n",
      "tensor(0.7728, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 102, Loss 0.7032595872879028: \n",
      "tensor(0.7033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 103, Loss 0.8299967646598816: \n",
      "tensor(0.8300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 104, Loss 0.6916919946670532: \n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 105, Loss 0.9484674334526062: \n",
      "tensor(0.9485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 106, Loss 0.7458885312080383: \n",
      "tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 107, Loss 0.6383867859840393: \n",
      "tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 108, Loss 0.8423703908920288: \n",
      "tensor(0.8424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 109, Loss 0.718434751033783: \n",
      "tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 110, Loss 0.7515553832054138: \n",
      "tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 111, Loss 0.7081132531166077: \n",
      "tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 112, Loss 0.7823240160942078: \n",
      "tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 113, Loss 0.9814358353614807: \n",
      "tensor(0.9814, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 114, Loss 0.7754554152488708: \n",
      "tensor(0.7755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 115, Loss 0.8036014437675476: \n",
      "tensor(0.8036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 116, Loss 0.8902607560157776: \n",
      "tensor(0.8903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 117, Loss 0.7298113107681274: \n",
      "tensor(0.7298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 118, Loss 0.9522021412849426: \n",
      "tensor(0.9522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 119, Loss 0.6931890845298767: \n",
      "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 120, Loss 0.8440208435058594: \n",
      "tensor(0.8440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 121, Loss 0.7088654637336731: \n",
      "tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 122, Loss 0.8582591414451599: \n",
      "tensor(0.8583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 123, Loss 0.7579811811447144: \n",
      "tensor(0.7580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 124, Loss 0.7271098494529724: \n",
      "tensor(0.7271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 125, Loss 0.7329278588294983: \n",
      "tensor(0.7329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 126, Loss 0.797544538974762: \n",
      "tensor(0.7975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 127, Loss 0.8080256581306458: \n",
      "tensor(0.8080, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 128, Loss 0.8264403343200684: \n",
      "tensor(0.8264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 129, Loss 0.7682371735572815: \n",
      "tensor(0.7682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 130, Loss 0.7882043123245239: \n",
      "tensor(0.7882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 131, Loss 1.0645228624343872: \n",
      "tensor(1.0645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 132, Loss 0.633853554725647: \n",
      "tensor(0.6339, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 133, Loss 0.7675293684005737: \n",
      "tensor(0.7675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 134, Loss 0.8429205417633057: \n",
      "tensor(0.8429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 135, Loss 0.6891658902168274: \n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 136, Loss 0.8520199060440063: \n",
      "tensor(0.8520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 137, Loss 0.9220975041389465: \n",
      "tensor(0.9221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 138, Loss 0.8063806891441345: \n",
      "tensor(0.8064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 139, Loss 0.9118524193763733: \n",
      "tensor(0.9119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 140, Loss 0.6621012091636658: \n",
      "tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 141, Loss 0.7648905515670776: \n",
      "tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 142, Loss 0.6920760273933411: \n",
      "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 143, Loss 0.746778666973114: \n",
      "tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 144, Loss 0.7034547328948975: \n",
      "tensor(0.7035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 145, Loss 0.8528661727905273: \n",
      "tensor(0.8529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 146, Loss 0.815771222114563: \n",
      "tensor(0.8158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 147, Loss 0.7659123539924622: \n",
      "tensor(0.7659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 148, Loss 0.9010689854621887: \n",
      "tensor(0.9011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 149, Loss 0.7477381229400635: \n",
      "tensor(0.7477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 150, Loss 0.8338055610656738: \n",
      "tensor(0.8338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 151, Loss 0.8462874293327332: \n",
      "tensor(0.8463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 152, Loss 0.8170737624168396: \n",
      "tensor(0.8171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 153, Loss 0.9212465286254883: \n",
      "tensor(0.9212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 154, Loss 0.7462272644042969: \n",
      "tensor(0.7462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 155, Loss 0.8580449819564819: \n",
      "tensor(0.8580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 156, Loss 0.7508153319358826: \n",
      "tensor(0.7508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 157, Loss 0.8869954347610474: \n",
      "tensor(0.8870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 158, Loss 0.6755403876304626: \n",
      "tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 159, Loss 0.8153469562530518: \n",
      "tensor(0.8153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 160, Loss 0.7337336540222168: \n",
      "tensor(0.7337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 161, Loss 0.5890153646469116: \n",
      "tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 162, Loss 0.7973612546920776: \n",
      "tensor(0.7974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 163, Loss 0.8141523599624634: \n",
      "tensor(0.8142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 164, Loss 0.6569714546203613: \n",
      "tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 165, Loss 0.7225326299667358: \n",
      "tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 166, Loss 0.7831550240516663: \n",
      "tensor(0.7832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 167, Loss 0.7744974493980408: \n",
      "tensor(0.7745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 168, Loss 0.8771390318870544: \n",
      "tensor(0.8771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 169, Loss 0.7523288726806641: \n",
      "tensor(0.7523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 170, Loss 0.7435674667358398: \n",
      "tensor(0.7436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 171, Loss 0.7459044456481934: \n",
      "tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 172, Loss 0.7675129771232605: \n",
      "tensor(0.7675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 173, Loss 0.8943501710891724: \n",
      "tensor(0.8944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 174, Loss 0.8959835767745972: \n",
      "tensor(0.8960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 175, Loss 0.7501634359359741: \n",
      "tensor(0.7502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 176, Loss 0.9335225820541382: \n",
      "tensor(0.9335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 177, Loss 0.7776366472244263: \n",
      "tensor(0.7776, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 178, Loss 0.9067387580871582: \n",
      "tensor(0.9067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 179, Loss 0.9062526822090149: \n",
      "tensor(0.9063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 180, Loss 0.9120036363601685: \n",
      "tensor(0.9120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 181, Loss 0.9501421451568604: \n",
      "tensor(0.9501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 182, Loss 0.8351042866706848: \n",
      "tensor(0.8351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 183, Loss 0.6657963991165161: \n",
      "tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 184, Loss 0.6734699010848999: \n",
      "tensor(0.6735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 185, Loss 0.8377820253372192: \n",
      "tensor(0.8378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 186, Loss 0.844359278678894: \n",
      "tensor(0.8444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 187, Loss 0.9065290093421936: \n",
      "tensor(0.9065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 188, Loss 0.8187245726585388: \n",
      "tensor(0.8187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 189, Loss 0.7234396934509277: \n",
      "tensor(0.7234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 190, Loss 0.652735710144043: \n",
      "tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 191, Loss 0.8902832865715027: \n",
      "tensor(0.8903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 192, Loss 0.8731483221054077: \n",
      "tensor(0.8731, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 193, Loss 0.8832478523254395: \n",
      "tensor(0.8832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 194, Loss 0.7959115505218506: \n",
      "tensor(0.7959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 195, Loss 0.809826672077179: \n",
      "tensor(0.8098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 196, Loss 0.801235020160675: \n",
      "tensor(0.8012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 197, Loss 0.871955931186676: \n",
      "tensor(0.8720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 198, Loss 0.7682779431343079: \n",
      "tensor(0.7683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 199, Loss 0.7370012402534485: \n",
      "tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 200, Loss 0.7461656928062439: \n",
      "tensor(0.7462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 201, Loss 0.6328194737434387: \n",
      "tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 202, Loss 1.0276026725769043: \n",
      "tensor(1.0276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 203, Loss 0.6769399642944336: \n",
      "tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 204, Loss 0.9536325931549072: \n",
      "tensor(0.9536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 205, Loss 0.695517897605896: \n",
      "tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 206, Loss 0.8268992900848389: \n",
      "tensor(0.8269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 207, Loss 0.6145007014274597: \n",
      "tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 208, Loss 0.7167494893074036: \n",
      "tensor(0.7167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 209, Loss 0.7723951935768127: \n",
      "tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 210, Loss 0.7711021304130554: \n",
      "tensor(0.7711, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 211, Loss 0.7928289771080017: \n",
      "tensor(0.7928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 212, Loss 0.8722653985023499: \n",
      "tensor(0.8723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 213, Loss 0.9185817241668701: \n",
      "tensor(0.9186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 214, Loss 0.8480174541473389: \n",
      "tensor(0.8480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 215, Loss 0.9149731397628784: \n",
      "tensor(0.9150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 216, Loss 0.7428121566772461: \n",
      "tensor(0.7428, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 217, Loss 0.6817132830619812: \n",
      "tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 218, Loss 0.7792699933052063: \n",
      "tensor(0.7793, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 219, Loss 0.7171948552131653: \n",
      "tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 220, Loss 0.7249084711074829: \n",
      "tensor(0.7249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 221, Loss 0.7280982732772827: \n",
      "tensor(0.7281, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 222, Loss 0.7934988141059875: \n",
      "tensor(0.7935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 223, Loss 0.7347569465637207: \n",
      "tensor(0.7348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 224, Loss 0.8337076306343079: \n",
      "tensor(0.8337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 225, Loss 0.6262825727462769: \n",
      "tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 226, Loss 0.8060310482978821: \n",
      "tensor(0.8060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 227, Loss 0.7589053511619568: \n",
      "tensor(0.7589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 228, Loss 1.0103837251663208: \n",
      "tensor(1.0104, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 229, Loss 0.8248245716094971: \n",
      "tensor(0.8248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 230, Loss 0.7984336614608765: \n",
      "tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 231, Loss 0.7558079361915588: \n",
      "tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 232, Loss 0.8367078900337219: \n",
      "tensor(0.8367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 233, Loss 0.7814804911613464: \n",
      "tensor(0.7815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 234, Loss 0.9951486587524414: \n",
      "tensor(0.9951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 235, Loss 0.7238184809684753: \n",
      "tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 236, Loss 0.9845637679100037: \n",
      "tensor(0.9846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 237, Loss 0.7532774209976196: \n",
      "tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 238, Loss 0.8029180765151978: \n",
      "tensor(0.8029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 239, Loss 0.7203892469406128: \n",
      "tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 240, Loss 0.7492793798446655: \n",
      "tensor(0.7493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 241, Loss 0.8339812755584717: \n",
      "tensor(0.8340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 242, Loss 0.8563985824584961: \n",
      "tensor(0.8564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 243, Loss 0.9139583110809326: \n",
      "tensor(0.9140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 244, Loss 0.6668058633804321: \n",
      "tensor(0.6668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 245, Loss 0.7778434753417969: \n",
      "tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 246, Loss 0.934735119342804: \n",
      "tensor(0.9347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 247, Loss 0.8693811297416687: \n",
      "tensor(0.8694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 248, Loss 0.792548418045044: \n",
      "tensor(0.7925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 249, Loss 0.6975727677345276: \n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 250, Loss 0.7697999477386475: \n",
      "tensor(0.7698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 251, Loss 0.8706879019737244: \n",
      "tensor(0.8707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 252, Loss 0.7067673206329346: \n",
      "tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 253, Loss 0.7018544673919678: \n",
      "tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 254, Loss 0.6711635589599609: \n",
      "tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 255, Loss 0.8614111542701721: \n",
      "tensor(0.8614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 256, Loss 0.8695040941238403: \n",
      "tensor(0.8695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 257, Loss 0.6628950834274292: \n",
      "tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 258, Loss 0.7352962493896484: \n",
      "tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 259, Loss 0.8800736665725708: \n",
      "tensor(0.8801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 260, Loss 0.7058692574501038: \n",
      "tensor(0.7059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 261, Loss 0.9307023286819458: \n",
      "tensor(0.9307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 262, Loss 0.8693618178367615: \n",
      "tensor(0.8694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 263, Loss 0.6879345178604126: \n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 264, Loss 0.7287209630012512: \n",
      "tensor(0.7287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 265, Loss 0.8066828846931458: \n",
      "tensor(0.8067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 266, Loss 0.8296161890029907: \n",
      "tensor(0.8296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 267, Loss 0.8826823830604553: \n",
      "tensor(0.8827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 268, Loss 0.8096219897270203: \n",
      "tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 269, Loss 0.9249182939529419: \n",
      "tensor(0.9249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 270, Loss 0.7801875472068787: \n",
      "tensor(0.7802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 271, Loss 0.8056912422180176: \n",
      "tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 272, Loss 0.8995227813720703: \n",
      "tensor(0.8995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 273, Loss 0.7885177135467529: \n",
      "tensor(0.7885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 274, Loss 0.8506852984428406: \n",
      "tensor(0.8507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 275, Loss 0.7035736441612244: \n",
      "tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 276, Loss 0.7352019548416138: \n",
      "tensor(0.7352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 277, Loss 0.83298259973526: \n",
      "tensor(0.8330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 278, Loss 0.7388695478439331: \n",
      "tensor(0.7389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 279, Loss 0.8496937155723572: \n",
      "tensor(0.8497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 280, Loss 0.896818995475769: \n",
      "tensor(0.8968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 281, Loss 0.6286606788635254: \n",
      "tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 282, Loss 0.7648493647575378: \n",
      "tensor(0.7648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 283, Loss 0.8870492577552795: \n",
      "tensor(0.8870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 284, Loss 0.9786756634712219: \n",
      "tensor(0.9787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 285, Loss 0.8682308197021484: \n",
      "tensor(0.8682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 286, Loss 0.8052639961242676: \n",
      "tensor(0.8053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 287, Loss 0.7646523118019104: \n",
      "tensor(0.7647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 288, Loss 0.7127953171730042: \n",
      "tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 289, Loss 0.6782058477401733: \n",
      "tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 290, Loss 0.9827153086662292: \n",
      "tensor(0.9827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 291, Loss 0.8423002362251282: \n",
      "tensor(0.8423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 292, Loss 0.8968366384506226: \n",
      "tensor(0.8968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 293, Loss 0.7778180837631226: \n",
      "tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 294, Loss 0.7274137139320374: \n",
      "tensor(0.7274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 295, Loss 0.7376869916915894: \n",
      "tensor(0.7377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 296, Loss 0.6995794177055359: \n",
      "tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 297, Loss 0.7723551392555237: \n",
      "tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 298, Loss 0.782974362373352: \n",
      "tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 299, Loss 0.7553210854530334: \n",
      "tensor(0.7553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 300, Loss 0.8808360695838928: \n",
      "tensor(0.8808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 301, Loss 0.7484953999519348: \n",
      "tensor(0.7485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 302, Loss 0.736953616142273: \n",
      "tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 303, Loss 0.8028352856636047: \n",
      "tensor(0.8028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 304, Loss 0.7814944386482239: \n",
      "tensor(0.7815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 305, Loss 0.8278429508209229: \n",
      "tensor(0.8278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 306, Loss 0.8941342830657959: \n",
      "tensor(0.8941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 307, Loss 0.7670326232910156: \n",
      "tensor(0.7670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 308, Loss 0.7726966142654419: \n",
      "tensor(0.7727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 309, Loss 0.7934087514877319: \n",
      "tensor(0.7934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 310, Loss 0.8144156336784363: \n",
      "tensor(0.8144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 311, Loss 0.712367057800293: \n",
      "tensor(0.7124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 312, Loss 0.8179467916488647: \n",
      "tensor(0.8179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 313, Loss 0.8298367857933044: \n",
      "tensor(0.8298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 314, Loss 0.8733283877372742: \n",
      "tensor(0.8733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 315, Loss 0.8500654697418213: \n",
      "tensor(0.8501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 316, Loss 0.8179466128349304: \n",
      "tensor(0.8179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 317, Loss 0.9130098819732666: \n",
      "tensor(0.9130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 318, Loss 0.644485592842102: \n",
      "tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 319, Loss 0.7589751482009888: \n",
      "tensor(0.7590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 320, Loss 0.6261898279190063: \n",
      "tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 321, Loss 0.8135631084442139: \n",
      "tensor(0.8136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 322, Loss 0.6729559898376465: \n",
      "tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 323, Loss 0.5928834080696106: \n",
      "tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 324, Loss 0.7905034422874451: \n",
      "tensor(0.7905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 325, Loss 0.8368682861328125: \n",
      "tensor(0.8369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 326, Loss 0.6642610430717468: \n",
      "tensor(0.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 327, Loss 0.7817103266716003: \n",
      "tensor(0.7817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 328, Loss 0.9193680882453918: \n",
      "tensor(0.9194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 329, Loss 0.651742160320282: \n",
      "tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 330, Loss 0.6741829514503479: \n",
      "tensor(0.6742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 331, Loss 0.9310868978500366: \n",
      "tensor(0.9311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 332, Loss 1.042177438735962: \n",
      "tensor(1.0422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 333, Loss 0.9116065502166748: \n",
      "tensor(0.9116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 334, Loss 0.9505351781845093: \n",
      "tensor(0.9505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 335, Loss 0.6599253416061401: \n",
      "tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 336, Loss 0.9447029232978821: \n",
      "tensor(0.9447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 337, Loss 0.8658474087715149: \n",
      "tensor(0.8658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 338, Loss 0.7133233547210693: \n",
      "tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 339, Loss 0.7741637229919434: \n",
      "tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 340, Loss 0.8329988718032837: \n",
      "tensor(0.8330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 341, Loss 0.8947583436965942: \n",
      "tensor(0.8948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 342, Loss 0.8205903172492981: \n",
      "tensor(0.8206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 343, Loss 0.842560887336731: \n",
      "tensor(0.8426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 344, Loss 0.8620650172233582: \n",
      "tensor(0.8621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 345, Loss 0.8821272253990173: \n",
      "tensor(0.8821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 346, Loss 0.8913336992263794: \n",
      "tensor(0.8913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 347, Loss 0.6090741157531738: \n",
      "tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 348, Loss 0.8244078755378723: \n",
      "tensor(0.8244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 349, Loss 0.8121060729026794: \n",
      "tensor(0.8121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 350, Loss 0.770570695400238: \n",
      "tensor(0.7706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 351, Loss 0.866161048412323: \n",
      "tensor(0.8662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 352, Loss 0.7245827317237854: \n",
      "tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 353, Loss 0.7022305727005005: \n",
      "tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 354, Loss 0.7225991487503052: \n",
      "tensor(0.7226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 355, Loss 0.8116695880889893: \n",
      "tensor(0.8117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 356, Loss 0.7484147548675537: \n",
      "tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 357, Loss 0.8440518975257874: \n",
      "tensor(0.8441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 358, Loss 0.8038663864135742: \n",
      "tensor(0.8039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 359, Loss 0.9371477961540222: \n",
      "tensor(0.9371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 360, Loss 0.7715441584587097: \n",
      "tensor(0.7715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 361, Loss 0.8594992756843567: \n",
      "tensor(0.8595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 362, Loss 0.8235663175582886: \n",
      "tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 363, Loss 0.7654951810836792: \n",
      "tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 364, Loss 0.6122186779975891: \n",
      "tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 365, Loss 0.8426384329795837: \n",
      "tensor(0.8426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 366, Loss 0.5641518235206604: \n",
      "tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 367, Loss 0.8109266757965088: \n",
      "tensor(0.8109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 368, Loss 0.8416503667831421: \n",
      "tensor(0.8417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 369, Loss 0.7804223895072937: \n",
      "tensor(0.7804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 370, Loss 0.6317780017852783: \n",
      "tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 371, Loss 0.7747803330421448: \n",
      "tensor(0.7748, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 372, Loss 0.8228012919425964: \n",
      "tensor(0.8228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 373, Loss 0.7339567542076111: \n",
      "tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 374, Loss 0.7569993138313293: \n",
      "tensor(0.7570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 375, Loss 0.9651520848274231: \n",
      "tensor(0.9652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 376, Loss 1.1075584888458252: \n",
      "tensor(1.1076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 377, Loss 0.7662783861160278: \n",
      "tensor(0.7663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 378, Loss 0.916860818862915: \n",
      "tensor(0.9169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 379, Loss 0.7895073890686035: \n",
      "tensor(0.7895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 380, Loss 0.7181845903396606: \n",
      "tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 381, Loss 0.7805392146110535: \n",
      "tensor(0.7805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 382, Loss 0.8484998941421509: \n",
      "tensor(0.8485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 383, Loss 0.8249326348304749: \n",
      "tensor(0.8249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 384, Loss 0.7694751024246216: \n",
      "tensor(0.7695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 385, Loss 0.8548767566680908: \n",
      "tensor(0.8549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 386, Loss 0.7083913683891296: \n",
      "tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 387, Loss 0.775486946105957: \n",
      "tensor(0.7755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 388, Loss 0.8460781574249268: \n",
      "tensor(0.8461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 389, Loss 0.7885186672210693: \n",
      "tensor(0.7885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 2, Iteration 390, Loss 0.9162226915359497: \n",
      "tensor(0.9162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 0, Loss 0.7777726054191589: \n",
      "tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 1, Loss 0.8725515007972717: \n",
      "tensor(0.8726, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 2, Loss 0.8194536566734314: \n",
      "tensor(0.8195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 3, Loss 0.7101399898529053: \n",
      "tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 4, Loss 0.7025683522224426: \n",
      "tensor(0.7026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 5, Loss 1.015488862991333: \n",
      "tensor(1.0155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 6, Loss 0.8030649423599243: \n",
      "tensor(0.8031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 7, Loss 0.9093806147575378: \n",
      "tensor(0.9094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 8, Loss 0.6240760087966919: \n",
      "tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 9, Loss 0.7500789165496826: \n",
      "tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 10, Loss 0.8056405186653137: \n",
      "tensor(0.8056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 11, Loss 0.6810055375099182: \n",
      "tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 12, Loss 0.7761824131011963: \n",
      "tensor(0.7762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 13, Loss 0.7782626748085022: \n",
      "tensor(0.7783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 14, Loss 0.851250171661377: \n",
      "tensor(0.8513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 15, Loss 0.7180229425430298: \n",
      "tensor(0.7180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 16, Loss 0.7583972811698914: \n",
      "tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 17, Loss 0.9115629196166992: \n",
      "tensor(0.9116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 18, Loss 0.8042798042297363: \n",
      "tensor(0.8043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 19, Loss 0.7439774870872498: \n",
      "tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 20, Loss 0.8462865352630615: \n",
      "tensor(0.8463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 21, Loss 0.6920453906059265: \n",
      "tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 22, Loss 0.7116971015930176: \n",
      "tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 23, Loss 0.6686500906944275: \n",
      "tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 24, Loss 0.7842189073562622: \n",
      "tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 25, Loss 0.7707782983779907: \n",
      "tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 26, Loss 0.7123689651489258: \n",
      "tensor(0.7124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 27, Loss 0.7357690930366516: \n",
      "tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 28, Loss 0.7675803303718567: \n",
      "tensor(0.7676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 29, Loss 0.7794367671012878: \n",
      "tensor(0.7794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 30, Loss 0.8534541726112366: \n",
      "tensor(0.8535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 31, Loss 0.6781487464904785: \n",
      "tensor(0.6781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 32, Loss 0.7853108644485474: \n",
      "tensor(0.7853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 33, Loss 0.713629961013794: \n",
      "tensor(0.7136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 34, Loss 0.6836337447166443: \n",
      "tensor(0.6836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 35, Loss 0.752216637134552: \n",
      "tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 36, Loss 0.660758376121521: \n",
      "tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 37, Loss 0.7899686098098755: \n",
      "tensor(0.7900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 38, Loss 0.888830304145813: \n",
      "tensor(0.8888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 39, Loss 0.6532644033432007: \n",
      "tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 40, Loss 0.6448202729225159: \n",
      "tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 41, Loss 0.7935191988945007: \n",
      "tensor(0.7935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 42, Loss 0.8329358696937561: \n",
      "tensor(0.8329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 43, Loss 0.6640293002128601: \n",
      "tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 44, Loss 0.7690288424491882: \n",
      "tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 45, Loss 0.7390708923339844: \n",
      "tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 46, Loss 0.5813085436820984: \n",
      "tensor(0.5813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 47, Loss 0.592586874961853: \n",
      "tensor(0.5926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 48, Loss 0.6894693970680237: \n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 49, Loss 0.9270931482315063: \n",
      "tensor(0.9271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 50, Loss 0.7194858193397522: \n",
      "tensor(0.7195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 51, Loss 0.7194948792457581: \n",
      "tensor(0.7195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 52, Loss 0.6673147678375244: \n",
      "tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 53, Loss 0.6940707564353943: \n",
      "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 54, Loss 0.6596540808677673: \n",
      "tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 55, Loss 0.739178478717804: \n",
      "tensor(0.7392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 56, Loss 0.7055795788764954: \n",
      "tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 57, Loss 0.6447667479515076: \n",
      "tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 58, Loss 0.7073506712913513: \n",
      "tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 59, Loss 0.8026436567306519: \n",
      "tensor(0.8026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 60, Loss 0.6421977877616882: \n",
      "tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 61, Loss 0.6897439956665039: \n",
      "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 62, Loss 0.8163267970085144: \n",
      "tensor(0.8163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 63, Loss 0.7270553708076477: \n",
      "tensor(0.7271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 64, Loss 0.7453486323356628: \n",
      "tensor(0.7453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 65, Loss 0.7183197736740112: \n",
      "tensor(0.7183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 66, Loss 0.7339717149734497: \n",
      "tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 67, Loss 0.7358004450798035: \n",
      "tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 68, Loss 0.8341485261917114: \n",
      "tensor(0.8341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 69, Loss 0.7402922511100769: \n",
      "tensor(0.7403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 70, Loss 0.7189114689826965: \n",
      "tensor(0.7189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 71, Loss 0.8870151042938232: \n",
      "tensor(0.8870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 72, Loss 0.5976447463035583: \n",
      "tensor(0.5976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 73, Loss 0.8191101551055908: \n",
      "tensor(0.8191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 74, Loss 0.7190935611724854: \n",
      "tensor(0.7191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 75, Loss 0.6661985516548157: \n",
      "tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 76, Loss 0.6380009651184082: \n",
      "tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 77, Loss 0.8752566576004028: \n",
      "tensor(0.8753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 78, Loss 0.5910844802856445: \n",
      "tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 79, Loss 0.6838335394859314: \n",
      "tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 80, Loss 0.6899901032447815: \n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 81, Loss 0.7465934157371521: \n",
      "tensor(0.7466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 82, Loss 0.7357742190361023: \n",
      "tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 83, Loss 0.7866587042808533: \n",
      "tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 84, Loss 0.6640315055847168: \n",
      "tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 85, Loss 0.6164206266403198: \n",
      "tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 86, Loss 0.8097077012062073: \n",
      "tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 87, Loss 0.9388768672943115: \n",
      "tensor(0.9389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 88, Loss 0.8045504093170166: \n",
      "tensor(0.8046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 89, Loss 0.6790556907653809: \n",
      "tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 90, Loss 0.6107056736946106: \n",
      "tensor(0.6107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 91, Loss 0.7392415404319763: \n",
      "tensor(0.7392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 92, Loss 0.6772739291191101: \n",
      "tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 93, Loss 0.7022601962089539: \n",
      "tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 94, Loss 0.7285094857215881: \n",
      "tensor(0.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 95, Loss 0.6801115274429321: \n",
      "tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 96, Loss 0.6990450620651245: \n",
      "tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 97, Loss 0.7595456838607788: \n",
      "tensor(0.7595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 98, Loss 0.7532508373260498: \n",
      "tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 99, Loss 0.6146954298019409: \n",
      "tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 100, Loss 0.6843733787536621: \n",
      "tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 101, Loss 0.6715090274810791: \n",
      "tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 102, Loss 0.6372460126876831: \n",
      "tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 103, Loss 0.7488218545913696: \n",
      "tensor(0.7488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 104, Loss 0.9039081931114197: \n",
      "tensor(0.9039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 105, Loss 0.6926553249359131: \n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 106, Loss 0.6387258172035217: \n",
      "tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 107, Loss 0.7557070851325989: \n",
      "tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 108, Loss 0.845001757144928: \n",
      "tensor(0.8450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 109, Loss 0.7873298525810242: \n",
      "tensor(0.7873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 110, Loss 0.7719491124153137: \n",
      "tensor(0.7719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 111, Loss 0.9430311918258667: \n",
      "tensor(0.9430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 112, Loss 0.6749327778816223: \n",
      "tensor(0.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 113, Loss 0.6785203218460083: \n",
      "tensor(0.6785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 114, Loss 0.6627331972122192: \n",
      "tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 115, Loss 0.625931978225708: \n",
      "tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 116, Loss 0.751888632774353: \n",
      "tensor(0.7519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 117, Loss 0.5396044254302979: \n",
      "tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 118, Loss 0.6179001331329346: \n",
      "tensor(0.6179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 119, Loss 0.7855021357536316: \n",
      "tensor(0.7855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 120, Loss 0.7549879550933838: \n",
      "tensor(0.7550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 121, Loss 0.8284745216369629: \n",
      "tensor(0.8285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 122, Loss 0.621366560459137: \n",
      "tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 123, Loss 0.7510004043579102: \n",
      "tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 124, Loss 0.6386929154396057: \n",
      "tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 125, Loss 0.6224372386932373: \n",
      "tensor(0.6224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 126, Loss 0.6821792125701904: \n",
      "tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 127, Loss 0.8640365600585938: \n",
      "tensor(0.8640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 128, Loss 0.7701455950737: \n",
      "tensor(0.7701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 129, Loss 0.7173115015029907: \n",
      "tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 130, Loss 0.5986190438270569: \n",
      "tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 131, Loss 0.6841627359390259: \n",
      "tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 132, Loss 0.7511149048805237: \n",
      "tensor(0.7511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 133, Loss 0.8570023775100708: \n",
      "tensor(0.8570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 134, Loss 0.6927002668380737: \n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 135, Loss 0.7559598088264465: \n",
      "tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 136, Loss 0.7257817387580872: \n",
      "tensor(0.7258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 137, Loss 0.7252228856086731: \n",
      "tensor(0.7252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 138, Loss 0.697025716304779: \n",
      "tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 139, Loss 0.7374721169471741: \n",
      "tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 140, Loss 0.7310785055160522: \n",
      "tensor(0.7311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 141, Loss 0.6732969880104065: \n",
      "tensor(0.6733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 142, Loss 0.8149691820144653: \n",
      "tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 143, Loss 0.7599200010299683: \n",
      "tensor(0.7599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 144, Loss 0.6962528824806213: \n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 145, Loss 0.6801446080207825: \n",
      "tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 146, Loss 0.808715283870697: \n",
      "tensor(0.8087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 147, Loss 0.8889902234077454: \n",
      "tensor(0.8890, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 148, Loss 0.687730073928833: \n",
      "tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 149, Loss 0.745701789855957: \n",
      "tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 150, Loss 0.6165074706077576: \n",
      "tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 151, Loss 0.7277963161468506: \n",
      "tensor(0.7278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 152, Loss 0.7183736562728882: \n",
      "tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 153, Loss 0.7265227437019348: \n",
      "tensor(0.7265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 154, Loss 1.0254461765289307: \n",
      "tensor(1.0254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 155, Loss 0.6645391583442688: \n",
      "tensor(0.6645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 156, Loss 0.6370290517807007: \n",
      "tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 157, Loss 0.5998715162277222: \n",
      "tensor(0.5999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 158, Loss 0.8024770021438599: \n",
      "tensor(0.8025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 159, Loss 0.5936726331710815: \n",
      "tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 160, Loss 0.6806513667106628: \n",
      "tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 161, Loss 0.823092520236969: \n",
      "tensor(0.8231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 162, Loss 0.7080480456352234: \n",
      "tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 163, Loss 0.6667688488960266: \n",
      "tensor(0.6668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 164, Loss 0.6430082321166992: \n",
      "tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 165, Loss 0.6460865139961243: \n",
      "tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 166, Loss 0.7696688771247864: \n",
      "tensor(0.7697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 167, Loss 0.5964948534965515: \n",
      "tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 168, Loss 0.6623088121414185: \n",
      "tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 169, Loss 0.8509182333946228: \n",
      "tensor(0.8509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 170, Loss 0.8155639171600342: \n",
      "tensor(0.8156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 171, Loss 0.7664666175842285: \n",
      "tensor(0.7665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 172, Loss 0.6963242292404175: \n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 173, Loss 0.8960020542144775: \n",
      "tensor(0.8960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 174, Loss 0.785416841506958: \n",
      "tensor(0.7854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 175, Loss 0.6895015835762024: \n",
      "tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 176, Loss 0.7816417217254639: \n",
      "tensor(0.7816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 177, Loss 0.8881155252456665: \n",
      "tensor(0.8881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 178, Loss 0.7072796821594238: \n",
      "tensor(0.7073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 179, Loss 0.7430328726768494: \n",
      "tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 180, Loss 0.866518497467041: \n",
      "tensor(0.8665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 181, Loss 0.7349725961685181: \n",
      "tensor(0.7350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 182, Loss 0.7118420600891113: \n",
      "tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 183, Loss 0.8174887299537659: \n",
      "tensor(0.8175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 184, Loss 0.7337061166763306: \n",
      "tensor(0.7337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 185, Loss 0.7874194383621216: \n",
      "tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 186, Loss 0.6081557273864746: \n",
      "tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 187, Loss 0.8423722386360168: \n",
      "tensor(0.8424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 188, Loss 0.8454740047454834: \n",
      "tensor(0.8455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 189, Loss 0.8800894618034363: \n",
      "tensor(0.8801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 190, Loss 0.6496046781539917: \n",
      "tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 191, Loss 0.7968724966049194: \n",
      "tensor(0.7969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 192, Loss 0.682641327381134: \n",
      "tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 193, Loss 0.6255273818969727: \n",
      "tensor(0.6255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 194, Loss 0.6439892649650574: \n",
      "tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 195, Loss 0.8124867677688599: \n",
      "tensor(0.8125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 196, Loss 0.7200323343276978: \n",
      "tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 197, Loss 0.7972742319107056: \n",
      "tensor(0.7973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 198, Loss 0.6815088987350464: \n",
      "tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 199, Loss 0.6049255728721619: \n",
      "tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 200, Loss 0.691122829914093: \n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 201, Loss 0.7887026071548462: \n",
      "tensor(0.7887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 202, Loss 0.8235626816749573: \n",
      "tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 203, Loss 0.6636219620704651: \n",
      "tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 204, Loss 0.7354334592819214: \n",
      "tensor(0.7354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 205, Loss 0.7340137362480164: \n",
      "tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 206, Loss 0.7573890089988708: \n",
      "tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 207, Loss 0.6171008944511414: \n",
      "tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 208, Loss 0.8649483919143677: \n",
      "tensor(0.8649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 209, Loss 0.79518723487854: \n",
      "tensor(0.7952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 210, Loss 0.7022310495376587: \n",
      "tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 211, Loss 0.8132067322731018: \n",
      "tensor(0.8132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 212, Loss 0.9218152165412903: \n",
      "tensor(0.9218, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 213, Loss 0.8021294474601746: \n",
      "tensor(0.8021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 214, Loss 0.8830728530883789: \n",
      "tensor(0.8831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 215, Loss 0.7540551424026489: \n",
      "tensor(0.7541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 216, Loss 0.7760940194129944: \n",
      "tensor(0.7761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 217, Loss 0.8151466250419617: \n",
      "tensor(0.8151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 218, Loss 0.6465017199516296: \n",
      "tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 219, Loss 0.8609747886657715: \n",
      "tensor(0.8610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 220, Loss 0.8955439925193787: \n",
      "tensor(0.8955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 221, Loss 0.5317405462265015: \n",
      "tensor(0.5317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 222, Loss 0.7128040194511414: \n",
      "tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 223, Loss 0.6678714752197266: \n",
      "tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 224, Loss 0.7773503661155701: \n",
      "tensor(0.7774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 225, Loss 0.7522674202919006: \n",
      "tensor(0.7523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 226, Loss 0.794330358505249: \n",
      "tensor(0.7943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 227, Loss 0.6431885957717896: \n",
      "tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 228, Loss 0.7674658894538879: \n",
      "tensor(0.7675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 229, Loss 0.9320117235183716: \n",
      "tensor(0.9320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 230, Loss 0.7668800354003906: \n",
      "tensor(0.7669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 231, Loss 0.7500298023223877: \n",
      "tensor(0.7500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 232, Loss 0.748692512512207: \n",
      "tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 233, Loss 0.6384986042976379: \n",
      "tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 234, Loss 0.6620658040046692: \n",
      "tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 235, Loss 0.7083994150161743: \n",
      "tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 236, Loss 0.8774397373199463: \n",
      "tensor(0.8774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 237, Loss 0.9541230797767639: \n",
      "tensor(0.9541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 238, Loss 0.8212124109268188: \n",
      "tensor(0.8212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 239, Loss 0.6833158731460571: \n",
      "tensor(0.6833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 240, Loss 0.7655801177024841: \n",
      "tensor(0.7656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 241, Loss 0.7244272232055664: \n",
      "tensor(0.7244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 242, Loss 0.7565762996673584: \n",
      "tensor(0.7566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 243, Loss 0.635222852230072: \n",
      "tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 244, Loss 0.7275136113166809: \n",
      "tensor(0.7275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 245, Loss 0.8158041834831238: \n",
      "tensor(0.8158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 246, Loss 0.6291337013244629: \n",
      "tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 247, Loss 0.7842040657997131: \n",
      "tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 248, Loss 0.6761685013771057: \n",
      "tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 249, Loss 0.7733559608459473: \n",
      "tensor(0.7734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 250, Loss 0.8532154560089111: \n",
      "tensor(0.8532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 251, Loss 0.8497754335403442: \n",
      "tensor(0.8498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 252, Loss 0.6160140633583069: \n",
      "tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 253, Loss 0.730672299861908: \n",
      "tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 254, Loss 1.0318046808242798: \n",
      "tensor(1.0318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 255, Loss 0.6284144520759583: \n",
      "tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 256, Loss 0.7395321130752563: \n",
      "tensor(0.7395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 257, Loss 0.7596219182014465: \n",
      "tensor(0.7596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 258, Loss 0.7526952624320984: \n",
      "tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 259, Loss 0.7559336423873901: \n",
      "tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 260, Loss 0.683121919631958: \n",
      "tensor(0.6831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 261, Loss 0.5857202410697937: \n",
      "tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 262, Loss 0.7415909171104431: \n",
      "tensor(0.7416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 263, Loss 0.6903502941131592: \n",
      "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 264, Loss 0.6767846345901489: \n",
      "tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 265, Loss 0.7141857147216797: \n",
      "tensor(0.7142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 266, Loss 0.8978727459907532: \n",
      "tensor(0.8979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 267, Loss 0.6860081553459167: \n",
      "tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 268, Loss 0.865624725818634: \n",
      "tensor(0.8656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 269, Loss 0.8440854549407959: \n",
      "tensor(0.8441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 270, Loss 0.7408354878425598: \n",
      "tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 271, Loss 0.7440774440765381: \n",
      "tensor(0.7441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 272, Loss 0.7603110074996948: \n",
      "tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 273, Loss 0.7576425671577454: \n",
      "tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 274, Loss 0.8520193696022034: \n",
      "tensor(0.8520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 275, Loss 0.9318086504936218: \n",
      "tensor(0.9318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 276, Loss 0.7542568445205688: \n",
      "tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 277, Loss 0.8125920295715332: \n",
      "tensor(0.8126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 278, Loss 0.7505233883857727: \n",
      "tensor(0.7505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 279, Loss 0.6547814607620239: \n",
      "tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 280, Loss 0.7535297274589539: \n",
      "tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 281, Loss 0.9274397492408752: \n",
      "tensor(0.9274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 282, Loss 0.8664124011993408: \n",
      "tensor(0.8664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 283, Loss 0.9202669858932495: \n",
      "tensor(0.9203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 284, Loss 0.8056908249855042: \n",
      "tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 285, Loss 0.5895443558692932: \n",
      "tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 286, Loss 0.7450830340385437: \n",
      "tensor(0.7451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 287, Loss 0.8958693146705627: \n",
      "tensor(0.8959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 288, Loss 0.8436964750289917: \n",
      "tensor(0.8437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 289, Loss 0.7570568323135376: \n",
      "tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 290, Loss 0.7866548895835876: \n",
      "tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 291, Loss 0.8014017939567566: \n",
      "tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 292, Loss 0.7960290312767029: \n",
      "tensor(0.7960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 293, Loss 0.6747876405715942: \n",
      "tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 294, Loss 0.7275794148445129: \n",
      "tensor(0.7276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 295, Loss 0.8206313848495483: \n",
      "tensor(0.8206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 296, Loss 0.8572492003440857: \n",
      "tensor(0.8572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 297, Loss 0.6431952714920044: \n",
      "tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 298, Loss 0.7664164900779724: \n",
      "tensor(0.7664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 299, Loss 0.7969692945480347: \n",
      "tensor(0.7970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 300, Loss 0.7290908694267273: \n",
      "tensor(0.7291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 301, Loss 0.5697418451309204: \n",
      "tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 302, Loss 0.9052920937538147: \n",
      "tensor(0.9053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 303, Loss 0.6183227300643921: \n",
      "tensor(0.6183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 304, Loss 0.7413468360900879: \n",
      "tensor(0.7413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 305, Loss 0.7301874160766602: \n",
      "tensor(0.7302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 306, Loss 0.7232612371444702: \n",
      "tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 307, Loss 0.6306236982345581: \n",
      "tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 308, Loss 0.7877690196037292: \n",
      "tensor(0.7878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 309, Loss 0.7869589924812317: \n",
      "tensor(0.7870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 310, Loss 0.7104958295822144: \n",
      "tensor(0.7105, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 311, Loss 0.742368757724762: \n",
      "tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 3, Iteration 312, Loss 0.7943978309631348: \n",
      "tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TRAINING_EPOCHS):\n\u001b[32m      4\u001b[39m     losses, accuracies = [], []\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Master CS\\Repositories\\ml_security_ws25\\.venv311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Master CS\\Repositories\\ml_security_ws25\\.venv311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Master CS\\Repositories\\ml_security_ws25\\.venv311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Master CS\\Repositories\\ml_security_ws25\\.venv311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Master CS\\Repositories\\ml_security_ws25\\.venv311\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[39m, in \u001b[36mCIFAR10.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    116\u001b[39m img = Image.fromarray(img)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    122\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Master CS\\Repositories\\ml_security_ws25\\.venv311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Master CS\\Repositories\\ml_security_ws25\\.venv311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    174\u001b[39m img = img.permute((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)).contiguous()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "TRAINING_EPOCHS = 10\n",
    "\n",
    "for epoch in range(TRAINING_EPOCHS):\n",
    "    losses, accuracies = [], []\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    " \n",
    "        Optimizer.zero_grad()\n",
    " \n",
    "        # forward + backward + optimize\n",
    "        outputs = training_net.forward(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        print(\"Epoch {}, Iteration {}, Loss {:.4f}: \".format(epoch, i,loss))\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "        \n",
    "    \n",
    "Optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c44c83-9788-4df8-8001-3fd8a8301882",
   "metadata": {},
   "source": [
    "### Task 5 – Test an image & interpret results (5P)\n",
    "1. Load the model with the saved weights to the GPU and put it in evaluation mode so we can use the model for inference. (1P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71ee30e8-e591-4052-95af-df5ebb15bf78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=8192, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save(training_net, directory: str, name: str):\n",
    "        \"\"\"Stores the current state and config of the agent\"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        path = os.path.join(directory, f'{name}.pt')\n",
    "        torch.save(training_net,path)\n",
    "        \n",
    "       \n",
    "save(training_net, \"./models\", \"simple_cnn_cifar10\")\n",
    "        \n",
    "training_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafea728-c479-4f7c-ae21-85db10ef7bc6",
   "metadata": {},
   "source": [
    "2. Take the first image from the *test* set and infer its class to see if the classifier was loaded properly. (2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5796be7b-7b38-434f-9454-455c5638c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_model = torch.load(\"./models/simple_cnn_cifar10.pt\", weights_only=False)\n",
    "first_image = test_dataset.data[:1]  # Shape (1, 32, 32, 3)\n",
    "\n",
    "# To Tensor \n",
    "first_image_tensor = torch.from_numpy(first_image).permute(0, 3, 1, 2).float() / 255.0  # -> (1, 3, 32, 32)\n",
    "\n",
    "# Normalize again\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.4914, 0.4822, 0.4465],\n",
    "    std=[0.2470, 0.2435, 0.2616]\n",
    ")\n",
    "first_image_tensor = normalize(first_image_tensor[0]).unsqueeze(0)  # Normalisierung + Batch-Dim\n",
    "\n",
    "# To GPU (Maybe needed? TODO when  )\n",
    "device = next(loaded_model.parameters()).device\n",
    "first_image_tensor = first_image_tensor.to(device)\n",
    "\n",
    "# Inference\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = loaded_model(first_image_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    loaded_model.softmax = torch.nn.Softmax(dim=1)\n",
    "    probabilities = loaded_model.softmax(output)\n",
    "    confidence_value = probabilities[0, predicted_class].item() * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735067fb-8cfd-4b10-9fab-c0d3139481b4",
   "metadata": {},
   "source": [
    "3. Display the image alongside the predicted label with its confidence as well as the label from the dataset. This visualizes the capability of the classifier to make a right decision. (2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5603767f-8123-4514-9f27-4fa8972da9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(first_image[0])\n",
    "plt.title(\"Predicted Class: {} \\n Confidence Value: {:.2f}%\".format(classes[int(predicted_class)], confidence_value))\n",
    "plt.axis('off');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
